{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "Loss_train_adam = []\n",
        "class MultiClassMLP:\n",
        "    def __init__(self, input_dim, hidden_dims, output_dim, activation='relu', random_seed=42):\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.output_dim = output_dim\n",
        "        self.layers = []   # weights and biases for each layer\n",
        "        self.num_layers = len(hidden_dims) + 1\n",
        "        parameters = {}\n",
        "\n",
        "        # Initialize weights and biases for each layer\n",
        "        np.random.seed(random_seed)\n",
        "        layer_dims = [input_dim] + hidden_dims + [output_dim]\n",
        "        self.activations = [\n",
        "            self._sigmoid if activation == 'sigmoid' else self._tanh if activation == 'tanh' else self._relu for i in\n",
        "            range(self.num_layers)]\n",
        "        for i in range(self.num_layers):\n",
        "            fan_in = layer_dims[i]\n",
        "            fan_out = layer_dims[i + 1]\n",
        "            W = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in)\n",
        "            b = np.zeros((1, fan_out))\n",
        "            \n",
        "            self.layers.append({'W': W, 'b': b})\n",
        "            \n",
        "    def find_range_weight():\n",
        "        max = 0\n",
        "        min = np.inf\n",
        "        for i in range(self.num_layers):\n",
        "            if max < np.max(self.layers[i]['W']):\n",
        "                max = np.max(self.layers[i]['W'])\n",
        "            if min > np.min(self.layers[i]['W']):\n",
        "                min = np.min(self.layers[i]['W'])\n",
        "        return max, min\n",
        "    \n",
        "\n",
        "    def update_parameters(self):\n",
        "        self.parameters = {}\n",
        "        for counter, i in enumerate(self.layers):\n",
        "            self.parameters.update({\"W\" + str(counter + 1): i[\"W\"], \"b\" + str(counter + 1): i[\"b\"]})\n",
        "\n",
        "    def update_layers(self):\n",
        "        for i in range(self.num_layers):\n",
        "            self.layers[i]['W'] = self.parameters[\"W\" + str(i + 1)]\n",
        "            self.layers[i]['b'] = self.parameters[\"b\" + str(i + 1)]\n",
        "\n",
        "    def _softmax(self, X):\n",
        "        exps = np.exp(X - np.max(X, axis=1, keepdims=True))\n",
        "        return exps / np.sum(exps, axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "    def _tanh(self, X):\n",
        "        return np.tanh(X)\n",
        "\n",
        "    def _sigmoid(self, X):\n",
        "        return 1 / (1 + np.exp(-X))\n",
        "\n",
        "    def _relu(self, X):\n",
        "        return np.maximum(0, X)\n",
        "\n",
        "    def _forward(self, X):\n",
        "        activations = [X]\n",
        "        for i in range(self.num_layers):\n",
        "            Z = np.dot(activations[-1], self.layers[i]['W']) + self.layers[i]['b']\n",
        "            A = self.activations[i](Z)\n",
        "            activations.append(A)\n",
        "        probs = self._softmax(activations[-1])  # Using softmax activation function for output layer\n",
        "        return activations, probs\n",
        "\n",
        "    def _backward(self, X, y, activations, probs, learning_rate):\n",
        "        grads = {}\n",
        "        dL_dO = probs - y\n",
        "        for i in reversed(range(self.num_layers)):\n",
        "            if self.activations[i] == self._sigmoid:\n",
        "                dA = dL_dO * activations[i + 1] * (1 - activations[i + 1])\n",
        "            elif self.activations[i] == self._tanh:\n",
        "                dA = dL_dO * (1 - activations[i + 1] ** 2)  # Derivative of tanh activation function\n",
        "            elif self.activations[i] == self._relu:\n",
        "                dA = dL_dO * np.where(activations[i + 1] > 0, 1, 0)  # Derivative of ReLU activation function\n",
        "            else:\n",
        "                raise ValueError(\"Invalid activation function\")\n",
        "\n",
        "            dZ = np.dot(dA, self.layers[i]['W'].T)\n",
        "            dW = np.dot(activations[i].T, dA)\n",
        "            db = np.sum(dA, axis=0, keepdims=True)\n",
        "            grads.update({\"dW\" + str(i + 1): dW, \"db\" + str(i + 1): db})\n",
        "            # self.layers[i]['W'] -= learning_rate * dW\n",
        "            # self.layers[i]['b'] -= learning_rate * db\n",
        "\n",
        "            dL_dO = dZ\n",
        "        return grads\n",
        "\n",
        "    def initialize_adam(self, parameters):\n",
        "        L = len(parameters) // 2\n",
        "        v = {}\n",
        "        s = {}\n",
        "        for l in range(L):\n",
        "            v[\"dW\" + str(l + 1)] = np.zeros(\n",
        "                (parameters[\"W\" + str(l + 1)].shape[0], parameters[\"W\" + str(l + 1)].shape[1]))\n",
        "            v[\"db\" + str(l + 1)] = np.zeros(\n",
        "                (parameters[\"b\" + str(l + 1)].shape[0], parameters[\"b\" + str(l + 1)].shape[1]))\n",
        "            s[\"dW\" + str(l + 1)] = np.zeros(\n",
        "                (parameters[\"W\" + str(l + 1)].shape[0], parameters[\"W\" + str(l + 1)].shape[1]))\n",
        "            s[\"db\" + str(l + 1)] = np.zeros(\n",
        "                (parameters[\"b\" + str(l + 1)].shape[0], parameters[\"b\" + str(l + 1)].shape[1]))\n",
        "        return v, s\n",
        "\n",
        "    def update_parameters_with_adam(self, parameters, grads, v, s, t, learning_rate=0.01, beta1=0.9, beta2=0.999,\n",
        "                                    epsilon=1e-8):\n",
        "        \"\"\"\n",
        "        Update parameters using Adam\n",
        "\n",
        "        Arguments:\n",
        "        parameters -- python dictionary containing your parameters:\n",
        "                        parameters['W' + str(l)] = Wl\n",
        "                        parameters['b' + str(l)] = bl\n",
        "        grads -- python dictionary containing your gradients for each parameters:\n",
        "                        grads['dW' + str(l)] = dWl\n",
        "                        grads['db' + str(l)] = dbl\n",
        "        v -- Adam variable, moving average of the first gradient, python dictionary\n",
        "        s -- Adam variable, moving average of the squared gradient, python dictionary\n",
        "        learning_rate -- the learning rate, scalar.\n",
        "        beta1 -- Exponential decay hyperparameter for the first moment estimates\n",
        "        beta2 -- Exponential decay hyperparameter for the second moment estimates\n",
        "        epsilon -- hyperparameter preventing division by zero in Adam updates\n",
        "\n",
        "        Returns:\n",
        "        parameters -- python dictionary containing your updated parameters\n",
        "        v -- Adam variable, moving average of the first gradient, python dictionary\n",
        "        s -- Adam variable, moving average of the squared gradient, python dictionary\n",
        "        \"\"\"\n",
        "\n",
        "        L = len(parameters) // 2  # number of layers in the neural networks\n",
        "        v_corrected = {}  # Initializing first moment estimate, python dictionary\n",
        "        s_corrected = {}  # Initializing second moment estimate, python dictionary\n",
        "\n",
        "        # Perform Adam update on all parameters\n",
        "        for l in range(L):\n",
        "            # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n",
        "            ### START CODE HERE ### (approx. 2 lines)\n",
        "            v[\"dW\" + str(l + 1)] = beta1 * v[\"dW\" + str(l + 1)] + (1 - beta1) * grads['dW' + str(l + 1)]\n",
        "            v[\"db\" + str(l + 1)] = beta1 * v[\"db\" + str(l + 1)] + (1 - beta1) * grads['db' + str(l + 1)]\n",
        "            ### END CODE HERE ###\n",
        "\n",
        "            # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n",
        "            ### START CODE HERE ### (approx. 2 lines)\n",
        "            v_corrected[\"dW\" + str(l + 1)] = v[\"dW\" + str(l + 1)] / (1 - beta1 ** t)\n",
        "            v_corrected[\"db\" + str(l + 1)] = v[\"db\" + str(l + 1)] / (1 - beta1 ** t)\n",
        "            ### END CODE HERE ###\n",
        "\n",
        "            # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n",
        "            ### START CODE HERE ### (approx. 2 lines)\n",
        "            s[\"dW\" + str(l + 1)] = beta2 * s[\"dW\" + str(l + 1)] + (1 - beta2) * np.square(grads['dW' + str(l + 1)])\n",
        "            s[\"db\" + str(l + 1)] = beta2 * s[\"db\" + str(l + 1)] + (1 - beta2) * np.square(grads['db' + str(l + 1)])\n",
        "            ### END CODE HERE ###\n",
        "\n",
        "            # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n",
        "            ### START CODE HERE ### (approx. 2 lines)\n",
        "            s_corrected[\"dW\" + str(l + 1)] = s[\"dW\" + str(l + 1)] / (1 - beta2 ** t)\n",
        "            s_corrected[\"db\" + str(l + 1)] = s[\"db\" + str(l + 1)] / (1 - beta2 ** t)\n",
        "            ### END CODE HERE ###\n",
        "\n",
        "            # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n",
        "            ### START CODE HERE ### (approx. 2 lines)\n",
        "            parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * v_corrected[\n",
        "                \"dW\" + str(l + 1)] / (np.sqrt(s_corrected[\"dW\" + str(l + 1)]) + epsilon)\n",
        "            parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * v_corrected[\n",
        "                \"db\" + str(l + 1)] / (np.sqrt(s_corrected[\"db\" + str(l + 1)]) + epsilon)\n",
        "            ### END CODE HERE ###\n",
        "\n",
        "        return parameters, v, s\n",
        "\n",
        "    def train(self, X_train, y_train, learning_rate=0.0007, mini_batch_size=64, beta=0.9,\n",
        "              beta1=0.9, beta2=0.999, epsilon=1e-8, num_epochs=10000, print_cost=True):\n",
        "        Y_train = np.eye(self.output_dim)[y_train]\n",
        "        t = 0\n",
        "        self.update_parameters()\n",
        "        v, s = self.initialize_adam(self.parameters)\n",
        "        for epoch in range(num_epochs):\n",
        "            activations, probs = self._forward(X_train)\n",
        "            grads = self._backward(X_train, Y_train, activations, probs, learning_rate)\n",
        "            self.update_parameters()\n",
        "            t = t + 1\n",
        "            self.parameters, v, s = self.update_parameters_with_adam(self.parameters, grads, v, s,\n",
        "                                                                t, learning_rate, beta1, beta2, epsilon)\n",
        "            self.update_layers()\n",
        "            loss = -np.sum(Y_train * np.log(probs)) / X_train.shape[0]\n",
        "            if epoch % 10 == 0:\n",
        "                Loss_train_adam.append(loss)\n",
        "                print(f\"Epoch {epoch}: Loss={loss:.4f}\")\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        _, probs = self._forward(X)\n",
        "        return probs\n",
        "\n",
        "    def predict(self, X):\n",
        "        probs = self.predict_proba(X)\n",
        "        return np.argmax(probs, axis=1)\n",
        "\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# torch is just for the feature extractor and the dataset (NOT FOR IMPLEMENTING NEURAL NETWORKS!)\n",
        "import torch\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet34\n",
        "import torch.nn as nn\n",
        "# sklearn is just for evaluation (NOT FOR IMPLEMENTING NEURAL NETWORKS!)\n",
        "from sklearn.metrics import confusion_matrix, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qMq81U3yFpD",
        "outputId": "a590d4cb-79e6-4c66-f0a3-83fa4bc1e50a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "feature_extractor = resnet34(pretrained=True)\n",
        "input_dim = feature_extractor.fc.in_features\n",
        "for param in feature_extractor.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "feature_extractor.fc = nn.Identity()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTwdpv59yGBC",
        "outputId": "274c1781-0488-4102-9a01-874672b195d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "train_data = datasets.CIFAR10('data', train=True,\n",
        "                              download=True, transform=transform)\n",
        "test_data = datasets.CIFAR10('data', train=False,\n",
        "                             download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hckp6Q-LyIhY"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oLQvp-OyLeT"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqxXXgD8yN1b",
        "outputId": "ba4c810a-d06d-4a84-ba68-cd8b8e62b7c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n"
          ]
        }
      ],
      "source": [
        "embeddings = []\n",
        "labels = []\n",
        "i = 0\n",
        "for x, y in train_loader:\n",
        "  if i == 50:\n",
        "    break\n",
        "  i += 1\n",
        "  print(i)\n",
        "  embeddings += feature_extractor(x)\n",
        "  labels += y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtvEpsKsy-Re",
        "outputId": "efb79756-f4fe-4df7-d858-e6e6ae1924b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        " len(next(iter(train_loader))[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmqQtpx1zd9V",
        "outputId": "9433bc45-5051-45de-b910-095d6f0a8ba4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3200"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAT7vrjl49wl",
        "outputId": "1978ab65-e543-4e7c-cd10-b201258e29cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[tensor(9),\n",
              " tensor(4),\n",
              " tensor(0),\n",
              " tensor(9),\n",
              " tensor(5),\n",
              " tensor(8),\n",
              " tensor(4),\n",
              " tensor(2),\n",
              " tensor(2),\n",
              " tensor(8),\n",
              " tensor(5),\n",
              " tensor(3),\n",
              " tensor(1),\n",
              " tensor(9),\n",
              " tensor(5),\n",
              " tensor(8),\n",
              " tensor(9),\n",
              " tensor(0),\n",
              " tensor(1),\n",
              " tensor(6),\n",
              " tensor(4),\n",
              " tensor(3),\n",
              " tensor(4),\n",
              " tensor(8),\n",
              " tensor(1),\n",
              " tensor(5),\n",
              " tensor(7),\n",
              " tensor(4),\n",
              " tensor(4),\n",
              " tensor(0),\n",
              " tensor(1),\n",
              " tensor(9),\n",
              " tensor(7),\n",
              " tensor(1),\n",
              " tensor(3),\n",
              " tensor(7),\n",
              " tensor(0),\n",
              " tensor(2),\n",
              " tensor(5),\n",
              " tensor(6),\n",
              " tensor(4),\n",
              " tensor(4),\n",
              " tensor(6),\n",
              " tensor(5),\n",
              " tensor(6),\n",
              " tensor(1),\n",
              " tensor(5),\n",
              " tensor(6),\n",
              " tensor(6),\n",
              " tensor(7),\n",
              " tensor(3),\n",
              " tensor(8),\n",
              " tensor(2),\n",
              " tensor(5),\n",
              " tensor(5),\n",
              " tensor(3),\n",
              " tensor(5),\n",
              " tensor(8),\n",
              " tensor(9),\n",
              " tensor(7),\n",
              " tensor(9),\n",
              " tensor(9),\n",
              " tensor(4),\n",
              " tensor(8),\n",
              " tensor(3),\n",
              " tensor(1),\n",
              " tensor(1),\n",
              " tensor(4),\n",
              " tensor(6),\n",
              " tensor(0),\n",
              " tensor(4),\n",
              " tensor(8),\n",
              " tensor(4),\n",
              " tensor(1),\n",
              " tensor(0),\n",
              " tensor(0),\n",
              " tensor(5),\n",
              " tensor(6),\n",
              " tensor(3),\n",
              " tensor(0),\n",
              " tensor(5),\n",
              " tensor(3),\n",
              " tensor(3),\n",
              " tensor(4),\n",
              " tensor(5),\n",
              " tensor(1),\n",
              " tensor(3),\n",
              " tensor(9),\n",
              " tensor(8),\n",
              " tensor(2),\n",
              " tensor(1),\n",
              " tensor(2),\n",
              " tensor(2),\n",
              " tensor(0),\n",
              " tensor(2),\n",
              " tensor(6),\n",
              " tensor(0),\n",
              " tensor(2),\n",
              " tensor(8),\n",
              " tensor(0),\n",
              " tensor(1),\n",
              " tensor(0),\n",
              " tensor(8),\n",
              " tensor(6),\n",
              " tensor(9),\n",
              " tensor(6),\n",
              " tensor(6),\n",
              " tensor(7),\n",
              " tensor(5),\n",
              " tensor(2),\n",
              " tensor(3),\n",
              " tensor(3),\n",
              " tensor(5),\n",
              " tensor(9),\n",
              " tensor(5),\n",
              " tensor(2),\n",
              " tensor(9),\n",
              " tensor(3),\n",
              " tensor(9),\n",
              " tensor(7),\n",
              " tensor(3),\n",
              " tensor(2),\n",
              " tensor(2),\n",
              " tensor(1),\n",
              " tensor(2),\n",
              " tensor(1),\n",
              " tensor(9),\n",
              " tensor(7),\n",
              " tensor(5),\n",
              " tensor(6),\n",
              " tensor(9),\n",
              " tensor(3),\n",
              " tensor(0),\n",
              " tensor(2),\n",
              " tensor(8),\n",
              " tensor(2),\n",
              " tensor(6),\n",
              " tensor(6),\n",
              " tensor(3),\n",
              " tensor(5),\n",
              " tensor(8),\n",
              " tensor(0),\n",
              " tensor(1),\n",
              " tensor(2),\n",
              " tensor(8),\n",
              " tensor(8),\n",
              " tensor(8),\n",
              " tensor(1),\n",
              " tensor(4),\n",
              " tensor(5),\n",
              " tensor(0),\n",
              " tensor(3),\n",
              " tensor(2),\n",
              " tensor(7),\n",
              " tensor(5),\n",
              " tensor(8),\n",
              " tensor(7),\n",
              " tensor(3),\n",
              " tensor(7),\n",
              " tensor(6),\n",
              " tensor(1),\n",
              " tensor(0),\n",
              " tensor(7),\n",
              " tensor(9),\n",
              " tensor(1),\n",
              " tensor(9),\n",
              " tensor(2),\n",
              " tensor(7),\n",
              " tensor(4),\n",
              " tensor(2),\n",
              " tensor(5),\n",
              " tensor(4),\n",
              " tensor(2),\n",
              " tensor(6),\n",
              " tensor(2),\n",
              " tensor(7),\n",
              " tensor(8),\n",
              " tensor(1),\n",
              " tensor(0),\n",
              " tensor(7),\n",
              " tensor(4),\n",
              " tensor(0),\n",
              " tensor(3),\n",
              " tensor(4),\n",
              " tensor(6),\n",
              " tensor(9),\n",
              " tensor(2),\n",
              " tensor(2),\n",
              " tensor(8),\n",
              " tensor(2),\n",
              " tensor(4),\n",
              " tensor(7),\n",
              " tensor(9),\n",
              " tensor(0),\n",
              " tensor(2),\n",
              " tensor(4),\n",
              " tensor(9),\n",
              " tensor(7),\n",
              " tensor(5),\n",
              " tensor(2),\n",
              " tensor(4),\n",
              " tensor(6),\n",
              " tensor(7),\n",
              " tensor(2),\n",
              " tensor(3),\n",
              " tensor(5),\n",
              " tensor(0),\n",
              " tensor(8),\n",
              " tensor(5),\n",
              " tensor(9),\n",
              " tensor(2),\n",
              " tensor(9),\n",
              " tensor(8),\n",
              " tensor(5),\n",
              " tensor(3),\n",
              " tensor(5),\n",
              " tensor(2),\n",
              " tensor(0),\n",
              " tensor(8),\n",
              " tensor(8),\n",
              " tensor(2),\n",
              " tensor(1),\n",
              " tensor(0),\n",
              " tensor(3),\n",
              " tensor(3),\n",
              " tensor(6),\n",
              " tensor(1),\n",
              " tensor(2),\n",
              " tensor(0),\n",
              " tensor(9),\n",
              " tensor(6),\n",
              " tensor(6),\n",
              " tensor(5),\n",
              " tensor(5),\n",
              " tensor(7),\n",
              " tensor(7),\n",
              " tensor(9),\n",
              " tensor(8),\n",
              " tensor(3),\n",
              " tensor(2),\n",
              " tensor(5),\n",
              " tensor(7),\n",
              " tensor(6),\n",
              " tensor(8),\n",
              " tensor(4),\n",
              " tensor(8),\n",
              " tensor(4),\n",
              " tensor(1),\n",
              " tensor(1),\n",
              " tensor(1),\n",
              " tensor(2),\n",
              " tensor(6),\n",
              " tensor(5),\n",
              " tensor(7),\n",
              " tensor(2),\n",
              " tensor(7),\n",
              " tensor(5),\n",
              " tensor(0),\n",
              " tensor(9),\n",
              " tensor(6),\n",
              " tensor(1),\n",
              " tensor(5),\n",
              " tensor(3),\n",
              " tensor(7),\n",
              " tensor(9),\n",
              " tensor(7),\n",
              " tensor(1),\n",
              " tensor(4),\n",
              " tensor(2),\n",
              " tensor(2),\n",
              " tensor(4),\n",
              " tensor(4),\n",
              " tensor(8),\n",
              " tensor(6),\n",
              " tensor(3),\n",
              " tensor(8),\n",
              " tensor(7),\n",
              " tensor(7),\n",
              " tensor(3),\n",
              " tensor(0),\n",
              " tensor(7),\n",
              " tensor(9),\n",
              " tensor(9),\n",
              " tensor(3),\n",
              " tensor(8),\n",
              " tensor(3),\n",
              " tensor(7),\n",
              " tensor(4),\n",
              " tensor(4),\n",
              " tensor(2),\n",
              " tensor(4),\n",
              " tensor(4),\n",
              " tensor(6),\n",
              " tensor(3),\n",
              " tensor(4),\n",
              " tensor(5),\n",
              " tensor(0),\n",
              " tensor(5),\n",
              " tensor(1),\n",
              " tensor(0),\n",
              " tensor(6),\n",
              " tensor(9),\n",
              " tensor(7),\n",
              " tensor(7),\n",
              " tensor(6),\n",
              " tensor(5),\n",
              " tensor(1),\n",
              " tensor(0),\n",
              " tensor(4),\n",
              " tensor(1),\n",
              " tensor(9),\n",
              " tensor(9),\n",
              " tensor(9),\n",
              " tensor(0),\n",
              " tensor(6),\n",
              " tensor(9),\n",
              " tensor(0),\n",
              " tensor(6),\n",
              " tensor(4),\n",
              " tensor(2),\n",
              " tensor(4),\n",
              " tensor(8),\n",
              " tensor(5),\n",
              " tensor(9),\n",
              " tensor(9),\n",
              " tensor(4),\n",
              " tensor(3),\n",
              " tensor(1),\n",
              " tensor(5),\n",
              " tensor(9),\n",
              " tensor(9),\n",
              " tensor(6),\n",
              " tensor(9),\n",
              " tensor(0),\n",
              " tensor(3),\n",
              " tensor(0),\n",
              " tensor(3),\n",
              " tensor(5),\n",
              " tensor(1),\n",
              " tensor(2),\n",
              " tensor(2),\n",
              " tensor(0),\n",
              " tensor(1),\n",
              " tensor(9),\n",
              " tensor(8),\n",
              " tensor(3),\n",
              " tensor(7),\n",
              " tensor(6),\n",
              " tensor(7),\n",
              " tensor(8),\n",
              " tensor(2),\n",
              " tensor(4),\n",
              " tensor(9),\n",
              " tensor(2),\n",
              " tensor(4),\n",
              " tensor(6),\n",
              " tensor(9),\n",
              " tensor(8),\n",
              " tensor(8),\n",
              " tensor(9),\n",
              " tensor(7),\n",
              " tensor(3),\n",
              " tensor(8),\n",
              " tensor(8),\n",
              " tensor(7),\n",
              " tensor(7),\n",
              " tensor(4),\n",
              " tensor(4),\n",
              " tensor(4),\n",
              " tensor(9),\n",
              " tensor(8),\n",
              " tensor(6),\n",
              " tensor(1),\n",
              " tensor(0),\n",
              " tensor(0),\n",
              " tensor(3),\n",
              " tensor(5),\n",
              " tensor(5),\n",
              " tensor(5),\n",
              " tensor(5),\n",
              " tensor(0),\n",
              " tensor(6),\n",
              " tensor(6),\n",
              " tensor(6),\n",
              " tensor(8),\n",
              " tensor(1),\n",
              " tensor(7),\n",
              " tensor(5),\n",
              " tensor(8),\n",
              " tensor(4),\n",
              " tensor(3),\n",
              " tensor(5),\n",
              " tensor(6),\n",
              " tensor(9),\n",
              " tensor(2),\n",
              " tensor(3),\n",
              " tensor(1),\n",
              " tensor(1),\n",
              " tensor(3),\n",
              " tensor(5),\n",
              " tensor(2),\n",
              " tensor(5),\n",
              " tensor(8),\n",
              " tensor(8),\n",
              " tensor(6),\n",
              " tensor(2),\n",
              " tensor(6),\n",
              " tensor(4),\n",
              " tensor(5),\n",
              " tensor(2),\n",
              " tensor(3),\n",
              " tensor(5),\n",
              " tensor(6),\n",
              " tensor(5),\n",
              " tensor(4),\n",
              " tensor(3),\n",
              " tensor(7),\n",
              " tensor(5),\n",
              " tensor(9),\n",
              " tensor(2),\n",
              " tensor(3),\n",
              " tensor(4),\n",
              " tensor(9),\n",
              " tensor(9),\n",
              " tensor(1),\n",
              " tensor(3),\n",
              " tensor(0),\n",
              " tensor(5),\n",
              " tensor(0),\n",
              " tensor(9),\n",
              " tensor(5),\n",
              " tensor(0),\n",
              " tensor(9),\n",
              " tensor(0),\n",
              " tensor(7),\n",
              " tensor(1),\n",
              " tensor(9),\n",
              " tensor(8),\n",
              " tensor(6),\n",
              " tensor(5),\n",
              " tensor(2),\n",
              " tensor(6),\n",
              " tensor(6),\n",
              " tensor(3),\n",
              " tensor(3),\n",
              " tensor(6),\n",
              " tensor(9),\n",
              " tensor(2),\n",
              " tensor(6),\n",
              " tensor(5),\n",
              " tensor(2),\n",
              " tensor(6),\n",
              " tensor(1),\n",
              " tensor(7),\n",
              " tensor(1),\n",
              " tensor(8),\n",
              " tensor(9),\n",
              " tensor(9),\n",
              " tensor(4),\n",
              " tensor(9),\n",
              " tensor(4),\n",
              " tensor(3),\n",
              " tensor(7),\n",
              " tensor(5),\n",
              " tensor(0),\n",
              " tensor(2),\n",
              " tensor(8),\n",
              " tensor(4),\n",
              " tensor(9),\n",
              " tensor(8),\n",
              " tensor(3),\n",
              " tensor(9),\n",
              " tensor(8),\n",
              " tensor(9),\n",
              " tensor(1),\n",
              " tensor(2),\n",
              " tensor(1),\n",
              " tensor(3),\n",
              " tensor(6),\n",
              " tensor(4),\n",
              " tensor(8),\n",
              " tensor(2),\n",
              " tensor(6),\n",
              " tensor(6),\n",
              " tensor(3),\n",
              " tensor(3),\n",
              " tensor(6),\n",
              " tensor(5),\n",
              " tensor(2),\n",
              " tensor(1),\n",
              " tensor(4),\n",
              " tensor(6),\n",
              " tensor(6),\n",
              " tensor(7),\n",
              " tensor(9),\n",
              " tensor(0),\n",
              " tensor(7),\n",
              " tensor(7),\n",
              " tensor(3),\n",
              " tensor(7),\n",
              " tensor(5),\n",
              " tensor(3),\n",
              " tensor(3),\n",
              " tensor(8),\n",
              " tensor(3),\n",
              " tensor(0),\n",
              " tensor(3),\n",
              " tensor(7),\n",
              " tensor(0),\n",
              " tensor(1),\n",
              " tensor(5),\n",
              " tensor(3),\n",
              " tensor(9),\n",
              " tensor(2),\n",
              " tensor(1),\n",
              " tensor(2),\n",
              " tensor(3),\n",
              " tensor(5),\n",
              " tensor(1),\n",
              " tensor(4),\n",
              " tensor(9),\n",
              " tensor(2),\n",
              " tensor(4),\n",
              " tensor(6),\n",
              " tensor(5),\n",
              " tensor(8),\n",
              " tensor(0),\n",
              " tensor(3),\n",
              " tensor(5),\n",
              " tensor(3),\n",
              " tensor(6),\n",
              " tensor(5),\n",
              " tensor(4),\n",
              " tensor(3),\n",
              " tensor(4),\n",
              " tensor(9),\n",
              " tensor(1),\n",
              " tensor(3),\n",
              " tensor(3),\n",
              " tensor(7),\n",
              " tensor(8),\n",
              " tensor(8),\n",
              " tensor(7),\n",
              " tensor(6),\n",
              " tensor(7),\n",
              " tensor(0),\n",
              " tensor(4),\n",
              " tensor(6),\n",
              " tensor(4),\n",
              " tensor(5),\n",
              " tensor(9),\n",
              " tensor(5),\n",
              " tensor(6),\n",
              " tensor(3),\n",
              " tensor(7),\n",
              " tensor(6),\n",
              " tensor(9),\n",
              " tensor(1),\n",
              " tensor(0),\n",
              " tensor(1),\n",
              " tensor(7),\n",
              " tensor(1),\n",
              " tensor(9),\n",
              " tensor(4),\n",
              " tensor(5),\n",
              " tensor(1),\n",
              " tensor(8),\n",
              " tensor(4),\n",
              " tensor(2),\n",
              " tensor(1),\n",
              " tensor(3),\n",
              " tensor(9),\n",
              " tensor(9),\n",
              " tensor(2),\n",
              " tensor(1),\n",
              " tensor(8),\n",
              " tensor(1),\n",
              " tensor(4),\n",
              " tensor(1),\n",
              " tensor(0),\n",
              " tensor(6),\n",
              " tensor(1),\n",
              " tensor(0),\n",
              " tensor(2),\n",
              " tensor(0),\n",
              " tensor(6),\n",
              " tensor(5),\n",
              " tensor(9),\n",
              " tensor(2),\n",
              " tensor(3),\n",
              " tensor(4),\n",
              " tensor(0),\n",
              " tensor(3),\n",
              " tensor(1),\n",
              " tensor(6),\n",
              " tensor(0),\n",
              " tensor(2),\n",
              " tensor(0),\n",
              " tensor(3),\n",
              " tensor(3),\n",
              " tensor(2),\n",
              " tensor(8),\n",
              " tensor(6),\n",
              " tensor(0),\n",
              " tensor(1),\n",
              " tensor(7),\n",
              " tensor(9),\n",
              " tensor(9),\n",
              " tensor(5),\n",
              " tensor(0),\n",
              " tensor(2),\n",
              " tensor(9),\n",
              " tensor(3),\n",
              " tensor(8),\n",
              " tensor(8),\n",
              " tensor(2),\n",
              " tensor(7),\n",
              " tensor(3),\n",
              " tensor(2),\n",
              " tensor(9),\n",
              " tensor(6),\n",
              " tensor(1),\n",
              " tensor(8),\n",
              " tensor(9),\n",
              " tensor(1),\n",
              " tensor(0),\n",
              " tensor(1),\n",
              " tensor(2),\n",
              " tensor(6),\n",
              " tensor(9),\n",
              " tensor(5),\n",
              " tensor(5),\n",
              " tensor(0),\n",
              " tensor(2),\n",
              " tensor(6),\n",
              " tensor(3),\n",
              " tensor(7),\n",
              " tensor(5),\n",
              " tensor(7),\n",
              " tensor(3),\n",
              " tensor(3),\n",
              " tensor(7),\n",
              " tensor(0),\n",
              " tensor(6),\n",
              " tensor(6),\n",
              " tensor(8),\n",
              " tensor(3),\n",
              " tensor(0),\n",
              " tensor(9),\n",
              " tensor(0),\n",
              " tensor(4),\n",
              " tensor(2),\n",
              " tensor(6),\n",
              " tensor(7),\n",
              " tensor(6),\n",
              " tensor(4),\n",
              " tensor(9),\n",
              " tensor(7),\n",
              " tensor(7),\n",
              " tensor(9),\n",
              " tensor(8),\n",
              " tensor(7),\n",
              " tensor(3),\n",
              " tensor(1),\n",
              " tensor(9),\n",
              " tensor(3),\n",
              " tensor(5),\n",
              " tensor(4),\n",
              " tensor(7),\n",
              " tensor(2),\n",
              " tensor(2),\n",
              " tensor(6),\n",
              " tensor(8),\n",
              " tensor(5),\n",
              " tensor(6),\n",
              " tensor(0),\n",
              " tensor(2),\n",
              " tensor(1),\n",
              " tensor(4),\n",
              " tensor(4),\n",
              " tensor(2),\n",
              " tensor(6),\n",
              " tensor(2),\n",
              " tensor(5),\n",
              " tensor(1),\n",
              " tensor(7),\n",
              " tensor(9),\n",
              " tensor(8),\n",
              " tensor(1),\n",
              " tensor(2),\n",
              " tensor(9),\n",
              " tensor(4),\n",
              " tensor(1),\n",
              " tensor(1),\n",
              " tensor(3),\n",
              " tensor(8),\n",
              " tensor(0),\n",
              " tensor(5),\n",
              " tensor(8),\n",
              " tensor(5),\n",
              " tensor(8),\n",
              " tensor(1),\n",
              " tensor(0),\n",
              " tensor(9),\n",
              " tensor(3),\n",
              " tensor(2),\n",
              " tensor(3),\n",
              " tensor(0),\n",
              " tensor(1),\n",
              " tensor(0),\n",
              " tensor(2),\n",
              " tensor(1),\n",
              " tensor(7),\n",
              " tensor(6),\n",
              " tensor(0),\n",
              " tensor(9),\n",
              " tensor(4),\n",
              " tensor(8),\n",
              " tensor(4),\n",
              " tensor(0),\n",
              " tensor(3),\n",
              " tensor(5),\n",
              " tensor(2),\n",
              " tensor(4),\n",
              " tensor(6),\n",
              " tensor(0),\n",
              " tensor(1),\n",
              " tensor(1),\n",
              " tensor(5),\n",
              " tensor(3),\n",
              " tensor(6),\n",
              " tensor(4),\n",
              " tensor(9),\n",
              " tensor(6),\n",
              " tensor(4),\n",
              " tensor(1),\n",
              " tensor(2),\n",
              " tensor(2),\n",
              " tensor(6),\n",
              " tensor(7),\n",
              " tensor(5),\n",
              " tensor(9),\n",
              " tensor(5),\n",
              " tensor(3),\n",
              " tensor(0),\n",
              " tensor(6),\n",
              " tensor(7),\n",
              " tensor(9),\n",
              " tensor(3),\n",
              " tensor(9),\n",
              " tensor(5),\n",
              " tensor(8),\n",
              " tensor(5),\n",
              " tensor(3),\n",
              " tensor(0),\n",
              " tensor(9),\n",
              " tensor(2),\n",
              " tensor(7),\n",
              " tensor(0),\n",
              " tensor(6),\n",
              " tensor(5),\n",
              " tensor(7),\n",
              " tensor(2),\n",
              " tensor(3),\n",
              " tensor(6),\n",
              " tensor(3),\n",
              " tensor(7),\n",
              " tensor(6),\n",
              " tensor(4),\n",
              " tensor(4),\n",
              " tensor(8),\n",
              " tensor(6),\n",
              " tensor(1),\n",
              " tensor(8),\n",
              " tensor(7),\n",
              " tensor(4),\n",
              " tensor(6),\n",
              " tensor(5),\n",
              " tensor(9),\n",
              " tensor(6),\n",
              " tensor(9),\n",
              " tensor(7),\n",
              " tensor(4),\n",
              " tensor(9),\n",
              " tensor(5),\n",
              " tensor(7),\n",
              " tensor(2),\n",
              " tensor(8),\n",
              " tensor(5),\n",
              " tensor(8),\n",
              " tensor(5),\n",
              " tensor(7),\n",
              " tensor(3),\n",
              " tensor(5),\n",
              " tensor(6),\n",
              " tensor(3),\n",
              " tensor(2),\n",
              " tensor(9),\n",
              " tensor(2),\n",
              " tensor(1),\n",
              " tensor(2),\n",
              " tensor(3),\n",
              " tensor(9),\n",
              " tensor(2),\n",
              " tensor(4),\n",
              " tensor(3),\n",
              " tensor(9),\n",
              " tensor(9),\n",
              " tensor(0),\n",
              " tensor(1),\n",
              " tensor(6),\n",
              " tensor(8),\n",
              " tensor(2),\n",
              " tensor(5),\n",
              " tensor(3),\n",
              " tensor(5),\n",
              " tensor(5),\n",
              " tensor(5),\n",
              " tensor(3),\n",
              " tensor(8),\n",
              " tensor(8),\n",
              " tensor(7),\n",
              " tensor(2),\n",
              " tensor(0),\n",
              " tensor(8),\n",
              " tensor(8),\n",
              " tensor(5),\n",
              " tensor(2),\n",
              " tensor(0),\n",
              " tensor(4),\n",
              " tensor(9),\n",
              " tensor(0),\n",
              " tensor(1),\n",
              " tensor(6),\n",
              " tensor(9),\n",
              " tensor(4),\n",
              " tensor(1),\n",
              " tensor(0),\n",
              " tensor(4),\n",
              " tensor(8),\n",
              " tensor(5),\n",
              " tensor(2),\n",
              " tensor(3),\n",
              " tensor(1),\n",
              " tensor(5),\n",
              " tensor(2),\n",
              " tensor(8),\n",
              " tensor(1),\n",
              " tensor(9),\n",
              " tensor(9),\n",
              " tensor(1),\n",
              " tensor(4),\n",
              " tensor(5),\n",
              " tensor(3),\n",
              " tensor(0),\n",
              " tensor(9),\n",
              " tensor(1),\n",
              " tensor(4),\n",
              " tensor(2),\n",
              " tensor(9),\n",
              " tensor(3),\n",
              " tensor(6),\n",
              " tensor(4),\n",
              " tensor(6),\n",
              " tensor(5),\n",
              " tensor(7),\n",
              " tensor(9),\n",
              " tensor(1),\n",
              " tensor(0),\n",
              " tensor(5),\n",
              " tensor(9),\n",
              " tensor(9),\n",
              " tensor(1),\n",
              " tensor(8),\n",
              " tensor(5),\n",
              " tensor(8),\n",
              " tensor(6),\n",
              " tensor(7),\n",
              " tensor(7),\n",
              " tensor(3),\n",
              " tensor(8),\n",
              " tensor(9),\n",
              " tensor(2),\n",
              " tensor(6),\n",
              " tensor(6),\n",
              " tensor(1),\n",
              " tensor(7),\n",
              " tensor(3),\n",
              " tensor(9),\n",
              " tensor(5),\n",
              " tensor(7),\n",
              " tensor(5),\n",
              " tensor(4),\n",
              " tensor(9),\n",
              " tensor(6),\n",
              " tensor(3),\n",
              " tensor(4),\n",
              " tensor(8),\n",
              " tensor(5),\n",
              " tensor(6),\n",
              " tensor(3),\n",
              " tensor(5),\n",
              " tensor(2),\n",
              " tensor(6),\n",
              " tensor(6),\n",
              " tensor(0),\n",
              " tensor(2),\n",
              " tensor(7),\n",
              " tensor(3),\n",
              " tensor(3),\n",
              " tensor(5),\n",
              " tensor(3),\n",
              " tensor(3),\n",
              " tensor(1),\n",
              " tensor(5),\n",
              " tensor(5),\n",
              " tensor(1),\n",
              " tensor(3),\n",
              " tensor(0),\n",
              " tensor(0),\n",
              " tensor(4),\n",
              " tensor(4),\n",
              " tensor(5),\n",
              " tensor(5),\n",
              " tensor(7),\n",
              " tensor(6),\n",
              " tensor(9),\n",
              " tensor(7),\n",
              " tensor(4),\n",
              " tensor(3),\n",
              " tensor(5),\n",
              " tensor(9),\n",
              " tensor(4),\n",
              " tensor(0),\n",
              " tensor(0),\n",
              " tensor(1),\n",
              " tensor(9),\n",
              " tensor(4),\n",
              " tensor(6),\n",
              " tensor(4),\n",
              " tensor(2),\n",
              " tensor(6),\n",
              " tensor(6),\n",
              " tensor(0),\n",
              " tensor(4),\n",
              " tensor(9),\n",
              " tensor(8),\n",
              " tensor(3),\n",
              " tensor(6),\n",
              " tensor(3),\n",
              " tensor(6),\n",
              " tensor(3),\n",
              " tensor(1),\n",
              " tensor(0),\n",
              " tensor(9),\n",
              " tensor(8),\n",
              " tensor(6),\n",
              " tensor(2),\n",
              " tensor(7),\n",
              " tensor(5),\n",
              " tensor(1),\n",
              " tensor(4),\n",
              " tensor(3),\n",
              " tensor(3),\n",
              " tensor(9),\n",
              " tensor(5),\n",
              " tensor(7),\n",
              " tensor(1),\n",
              " tensor(4),\n",
              " tensor(2),\n",
              " tensor(1),\n",
              " tensor(7),\n",
              " tensor(8),\n",
              " tensor(8),\n",
              " tensor(1),\n",
              " tensor(5),\n",
              " tensor(9),\n",
              " tensor(1),\n",
              " tensor(8),\n",
              " tensor(6),\n",
              " tensor(9),\n",
              " tensor(2),\n",
              " tensor(7),\n",
              " tensor(7),\n",
              " tensor(4),\n",
              " tensor(7),\n",
              " tensor(6),\n",
              " tensor(8),\n",
              " tensor(0),\n",
              " tensor(2),\n",
              " tensor(9),\n",
              " tensor(4),\n",
              " tensor(3),\n",
              " tensor(3),\n",
              " tensor(3),\n",
              " tensor(7),\n",
              " tensor(5),\n",
              " tensor(1),\n",
              " tensor(8),\n",
              " tensor(6),\n",
              " ...]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9W9yPMj4uql",
        "outputId": "80a002bc-3c4f-4706-a882-3407d3bfe349"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-29-e56cbe7bc52a>:4: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
            "  X = np.array(embeddings)\n",
            "<ipython-input-29-e56cbe7bc52a>:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X = np.array(embeddings)\n"
          ]
        }
      ],
      "source": [
        "# data_tensor = torch.cat(embeddings, dim=0)\n",
        "\n",
        "# Convert the tensor to a NumPy array\n",
        "X = np.array(embeddings)\n",
        "# data_tensor = torch.cat(labels, dim=0)\n",
        "for i in range(len(X)):\n",
        "  X[i] = np.array(X[i])\n",
        "# Convert the tensor to a NumPy array\n",
        "Y = np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Txs29uJ5pf2",
        "outputId": "8e38c21c-5466-439c-8c3f-8ecc2942838c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3200, 512)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result = []\n",
        "for i in X:\n",
        "  result.append(i)\n",
        "result = np.array(result)\n",
        "result.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nyz10IPmD-FC",
        "outputId": "36a60f90-1269-4ee1-d942-6b52bf391670"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3200,)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Y.flatten().shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nOciu92XM2k",
        "outputId": "7b505d9a-5cf5-4d5a-a44e-e2af2d89b5e1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.999375"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "clf = MLPClassifier(random_state=1, max_iter=1000, hidden_layer_sizes=[20]).fit(result, Y)\n",
        "clf.score(result, Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chart():\n",
        "    # Generate x-axis values using range() function\n",
        "    x = range(len(Loss_train_adam))\n",
        "\n",
        "    # Create line plot\n",
        "    plt.plot(x, Loss_train_adam)\n",
        "\n",
        "    # Add labels and title\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss vs. Epochs for Adam Optimizer')\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITizemtm2TzR",
        "outputId": "aa6ff0f0-596b-4719-a337-f56e6f420205"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: Loss=2.4393\n",
            "Epoch 10: Loss=2.0064\n",
            "Epoch 20: Loss=1.7976\n",
            "Epoch 30: Loss=1.6791\n",
            "Epoch 40: Loss=1.5942\n",
            "Epoch 50: Loss=1.5248\n",
            "Epoch 60: Loss=1.4646\n",
            "Epoch 70: Loss=1.4112\n",
            "Epoch 80: Loss=1.3632\n",
            "Epoch 90: Loss=1.3196\n",
            "Epoch 100: Loss=1.2796\n",
            "Epoch 110: Loss=1.2428\n",
            "Epoch 120: Loss=1.2094\n",
            "Epoch 130: Loss=1.1791\n",
            "Epoch 140: Loss=1.1516\n",
            "Epoch 150: Loss=1.1266\n",
            "Epoch 160: Loss=1.1037\n",
            "Epoch 170: Loss=1.0827\n",
            "Epoch 180: Loss=1.0634\n",
            "Epoch 190: Loss=1.0458\n",
            "Epoch 200: Loss=1.0298\n",
            "Epoch 210: Loss=1.0151\n",
            "Epoch 220: Loss=1.0015\n",
            "Epoch 230: Loss=0.9893\n",
            "Epoch 240: Loss=0.9783\n",
            "Epoch 250: Loss=0.9683\n",
            "Epoch 260: Loss=0.9592\n",
            "Epoch 270: Loss=0.9511\n",
            "Epoch 280: Loss=0.9436\n",
            "Epoch 290: Loss=0.9368\n",
            "Epoch 300: Loss=0.9304\n",
            "Epoch 310: Loss=0.9245\n",
            "Epoch 320: Loss=0.9193\n",
            "Epoch 330: Loss=0.9145\n",
            "Epoch 340: Loss=0.9102\n",
            "Epoch 350: Loss=0.9061\n",
            "Epoch 360: Loss=0.9024\n",
            "Epoch 370: Loss=0.8989\n",
            "Epoch 380: Loss=0.8956\n",
            "Epoch 390: Loss=0.8927\n",
            "Epoch 400: Loss=0.8900\n",
            "Epoch 410: Loss=0.8876\n",
            "Epoch 420: Loss=0.8853\n",
            "Epoch 430: Loss=0.8831\n",
            "Epoch 440: Loss=0.8810\n",
            "Epoch 450: Loss=0.8791\n",
            "Epoch 460: Loss=0.8773\n",
            "Epoch 470: Loss=0.8757\n",
            "Epoch 480: Loss=0.8741\n",
            "Epoch 490: Loss=0.8723\n",
            "Epoch 500: Loss=0.8708\n",
            "Epoch 510: Loss=0.8695\n",
            "Epoch 520: Loss=0.8682\n",
            "Epoch 530: Loss=0.8671\n",
            "Epoch 540: Loss=0.8660\n",
            "Epoch 550: Loss=0.8648\n",
            "Epoch 560: Loss=0.8638\n",
            "Epoch 570: Loss=0.8629\n",
            "Epoch 580: Loss=0.8620\n",
            "Epoch 590: Loss=0.8612\n",
            "Epoch 600: Loss=0.8604\n",
            "Epoch 610: Loss=0.8597\n",
            "Epoch 620: Loss=0.8590\n",
            "Epoch 630: Loss=0.8583\n",
            "Epoch 640: Loss=0.8575\n",
            "Epoch 650: Loss=0.8565\n",
            "Epoch 660: Loss=0.8557\n",
            "Epoch 670: Loss=0.8548\n",
            "Epoch 680: Loss=0.8541\n",
            "Epoch 690: Loss=0.8536\n",
            "Epoch 700: Loss=0.8531\n",
            "Epoch 710: Loss=0.8526\n",
            "Epoch 720: Loss=0.8522\n",
            "Epoch 730: Loss=0.8518\n",
            "Epoch 740: Loss=0.8515\n",
            "Epoch 750: Loss=0.8511\n",
            "Epoch 760: Loss=0.8507\n",
            "Epoch 770: Loss=0.8504\n",
            "Epoch 780: Loss=0.8501\n",
            "Epoch 790: Loss=0.8495\n",
            "Epoch 800: Loss=0.8491\n",
            "Epoch 810: Loss=0.8488\n",
            "Epoch 820: Loss=0.8484\n",
            "Epoch 830: Loss=0.8481\n",
            "Epoch 840: Loss=0.8478\n",
            "Epoch 850: Loss=0.8475\n",
            "Epoch 860: Loss=0.8473\n",
            "Epoch 870: Loss=0.8471\n",
            "Epoch 880: Loss=0.8468\n",
            "Epoch 890: Loss=0.8466\n",
            "Epoch 900: Loss=0.8464\n",
            "Epoch 910: Loss=0.8462\n",
            "Epoch 920: Loss=0.8460\n",
            "Epoch 930: Loss=0.8458\n",
            "Epoch 940: Loss=0.8455\n",
            "Epoch 950: Loss=0.8450\n",
            "Epoch 960: Loss=0.8448\n",
            "Epoch 970: Loss=0.8443\n",
            "Epoch 980: Loss=0.8437\n",
            "Epoch 990: Loss=0.8435\n"
          ]
        }
      ],
      "source": [
        "mlp = MultiClassMLP(input_dim=512, hidden_dims=[100], output_dim=10, activation=\"tanh\")\n",
        "mlp.train(result, Y, num_epochs=1000)\n",
        "chart()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArEkZbY-f8xY",
        "outputId": "f893b884-7733-49fd-f0e2-008d23569e12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9646875\n"
          ]
        }
      ],
      "source": [
        "counter = 0\n",
        "for i, t in enumerate(result):\n",
        "  if mlp.predict(t) == Y[i]:\n",
        "    counter += 1\n",
        "print(counter / len(result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(mlp.find_range_weight())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lel9G6Ip3cYg",
        "outputId": "5f75c4a1-fdfb-4344-f8a6-783b93aa7bd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: Loss=1.5996\n",
            "Epoch 10: Loss=1.5479\n",
            "Epoch 20: Loss=1.5098\n",
            "Epoch 30: Loss=1.4817\n",
            "Epoch 40: Loss=1.4593\n",
            "Epoch 50: Loss=1.4382\n",
            "Epoch 60: Loss=1.4175\n",
            "Epoch 70: Loss=1.3972\n",
            "Epoch 80: Loss=1.3778\n",
            "Epoch 90: Loss=1.3594\n",
            "Epoch 100: Loss=1.3424\n",
            "Epoch 110: Loss=1.3268\n",
            "Epoch 120: Loss=1.3122\n",
            "Epoch 130: Loss=1.2984\n",
            "Epoch 140: Loss=1.2854\n",
            "Epoch 150: Loss=1.2729\n",
            "Epoch 160: Loss=1.2605\n",
            "Epoch 170: Loss=1.2485\n",
            "Epoch 180: Loss=1.2379\n",
            "Epoch 190: Loss=1.2283\n",
            "Epoch 200: Loss=1.2195\n",
            "Epoch 210: Loss=1.2114\n",
            "Epoch 220: Loss=1.2039\n",
            "Epoch 230: Loss=1.1970\n",
            "Epoch 240: Loss=1.1905\n",
            "Epoch 250: Loss=1.1842\n",
            "Epoch 260: Loss=1.1781\n",
            "Epoch 270: Loss=1.1726\n",
            "Epoch 280: Loss=1.1675\n",
            "Epoch 290: Loss=1.1631\n",
            "Epoch 300: Loss=1.1593\n",
            "Epoch 310: Loss=1.1560\n",
            "Epoch 320: Loss=1.1531\n",
            "Epoch 330: Loss=1.1504\n",
            "Epoch 340: Loss=1.1480\n",
            "Epoch 350: Loss=1.1458\n",
            "Epoch 360: Loss=1.1438\n",
            "Epoch 370: Loss=1.1418\n",
            "Epoch 380: Loss=1.1399\n",
            "Epoch 390: Loss=1.1381\n",
            "Epoch 400: Loss=1.1364\n",
            "Epoch 410: Loss=1.1347\n",
            "Epoch 420: Loss=1.1332\n",
            "Epoch 430: Loss=1.1318\n",
            "Epoch 440: Loss=1.1305\n",
            "Epoch 450: Loss=1.1293\n",
            "Epoch 460: Loss=1.1280\n",
            "Epoch 470: Loss=1.1248\n",
            "Epoch 480: Loss=1.1216\n",
            "Epoch 490: Loss=1.1203\n",
            "Epoch 500: Loss=1.1192\n",
            "Epoch 510: Loss=1.1182\n",
            "Epoch 520: Loss=1.1174\n",
            "Epoch 530: Loss=1.1165\n",
            "Epoch 540: Loss=1.1157\n",
            "Epoch 550: Loss=1.1146\n",
            "Epoch 560: Loss=1.1134\n",
            "Epoch 570: Loss=1.1126\n",
            "Epoch 580: Loss=1.1118\n",
            "Epoch 590: Loss=1.1111\n",
            "Epoch 600: Loss=1.1101\n",
            "Epoch 610: Loss=1.1090\n",
            "Epoch 620: Loss=1.1081\n",
            "Epoch 630: Loss=1.1073\n",
            "Epoch 640: Loss=1.1066\n",
            "Epoch 650: Loss=1.1060\n",
            "Epoch 660: Loss=1.1054\n",
            "Epoch 670: Loss=1.1049\n",
            "Epoch 680: Loss=1.1043\n",
            "Epoch 690: Loss=1.1038\n",
            "Epoch 700: Loss=1.1032\n",
            "Epoch 710: Loss=1.1024\n",
            "Epoch 720: Loss=1.1011\n",
            "Epoch 730: Loss=1.1000\n",
            "Epoch 740: Loss=1.0993\n",
            "Epoch 750: Loss=1.0987\n",
            "Epoch 760: Loss=1.0982\n",
            "Epoch 770: Loss=1.0976\n",
            "Epoch 780: Loss=1.0972\n",
            "Epoch 790: Loss=1.0967\n",
            "Epoch 800: Loss=1.0963\n",
            "Epoch 810: Loss=1.0959\n",
            "Epoch 820: Loss=1.0955\n",
            "Epoch 830: Loss=1.0951\n",
            "Epoch 840: Loss=1.0947\n",
            "Epoch 850: Loss=1.0942\n",
            "Epoch 860: Loss=1.0936\n",
            "Epoch 870: Loss=1.0930\n",
            "Epoch 880: Loss=1.0926\n",
            "Epoch 890: Loss=1.0922\n",
            "Epoch 900: Loss=1.0919\n",
            "Epoch 910: Loss=1.0916\n",
            "Epoch 920: Loss=1.0912\n",
            "Epoch 930: Loss=1.0908\n",
            "Epoch 940: Loss=1.0903\n",
            "Epoch 950: Loss=1.0899\n",
            "Epoch 960: Loss=1.0896\n",
            "Epoch 970: Loss=1.0894\n",
            "Epoch 980: Loss=1.0891\n",
            "Epoch 990: Loss=1.0889\n"
          ]
        }
      ],
      "source": [
        "X_train = np.random.randn(100, 10)\n",
        "y_train = np.random.randint(0, 5, size=100)\n",
        "# Y_train = np.eye(5)[y_train]\n",
        "\n",
        "# Initialize and train the MLP\n",
        "mlp2 = MultiClassMLP(input_dim=10, hidden_dims=[20], output_dim=5, activation=\"sigmoid\")\n",
        "mlp2.train(X_train, y_train, learning_rate=0.1, num_epochs=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTcr5l_7Ufnc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
