{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load CIFAR10 training set without labels\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "cifar10_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "cifar10_loader = torch.utils.data.DataLoader(cifar10_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "for image, label in cifar10_loader:\n",
    "    images.append(image.squeeze().numpy())\n",
    "    labels.append(label)\n",
    "images = np.array(images)\n",
    "\n",
    "# Step 2: Load pre-trained 34ResNet network\n",
    "resnet = torchvision.models.resnet34(pretrained=True)\n",
    "resnet.eval()\n",
    "\n",
    "# Step 3: Extract feature vectors\n",
    "features = []\n",
    "for image in images:\n",
    "    image_tensor = torch.from_numpy(image).unsqueeze(0)\n",
    "    feature_vector = resnet(image_tensor)\n",
    "    features.append(feature_vector.squeeze().detach().numpy())\n",
    "features = np.array(features)\n",
    "\n",
    "# Step 4: Normalize the feature vectors using feature scaling\n",
    "min_vals = np.min(features, axis=0)\n",
    "max_vals = np.max(features, axis=0)\n",
    "normalized_features = (features - min_vals) / (max_vals - min_vals)\n",
    "\n",
    "# Step 5: Define SOM network parameters\n",
    "output_neurons = 10\n",
    "input_dim = normalized_features.shape[1]\n",
    "\n",
    "# Step 6: Initialize SOM weights with optimal values for each mode\n",
    "modes = [\n",
    "    {\n",
    "        \"name\": \"First mode\",\n",
    "        \"neighborhood_diameter\": 1,\n",
    "        \"weight_vectors\": normalized_features[::len(normalized_features) // output_neurons].reshape((output_neurons, -1))\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Second mode\",\n",
    "        \"neighborhood_diameter\": 3,\n",
    "        \"weight_vectors\": normalized_features[::len(normalized_features) // output_neurons].reshape((output_neurons, -1))\n",
    "    }\n",
    "]\n",
    "\n",
    "for mode in modes:\n",
    "    print(mode[\"name\"])\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Step 7: Train SOM network\n",
    "    epochs = 20\n",
    "    learning_rate = 0.5\n",
    "\n",
    "    # Initialize SOM weights with optimal values\n",
    "    weight_vectors = mode[\"weight_vectors\"]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Adjust learning rate and neighborhood diameter\n",
    "        current_learning_rate = learning_rate * (1 - epoch / epochs)\n",
    "        current_diameter = int(mode[\"neighborhood_diameter\"] * (1 - epoch / epochs))\n",
    "\n",
    "        for feature_vector in normalized_features:\n",
    "            # Find the winning neuron\n",
    "            distances = np.linalg.norm(feature_vector - weight_vectors, axis=1)\n",
    "            winner_neuron = np.argmin(distances)\n",
    "\n",
    "            # Update the winning neuron and its neighbors\n",
    "            for neuron in range(output_neurons):\n",
    "                distance = abs(neuron - winner_neuron)\n",
    "                if distance <= current_diameter:\n",
    "                    influence = np.exp(-(distance**2) / (2 * current_diameter**2))\n",
    "                    weight_vectors[neuron] += current_learning_rate * influence * (feature_vector - weight_vectors[neuron])\n",
    "\n",
    "    # Step 8: Determine distribution of labels in each cluster\n",
    "    cluster_labels = [[] for _ in range(output_neurons)]\n",
    "    for i, feature_vector in enumerate(normalized_features):\n",
    "        distances = np.linalg.norm(feature_vector - weight_vectors, axis=1)\n",
    "        winner_neuron = np.argmin(distances)\n",
    "        cluster_labels[winner_neuron].append(cifar10_dataset[i][1])\n",
    "\n",
    "    # Print the distribution of labels in each cluster\n",
    "    for i, labels in enumerate(cluster_labels):\n",
    "        print(f\"Cluster {i+1}: {len(labels)} images\")\n",
    "        label_counts = {label: labels.count(label) for label in set(labels)}\n",
    "        for label, count in label_counts.items():\n",
    "            print(f\"  Label {label}: {count} images\")\n",
    "        print()\n",
    "\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
