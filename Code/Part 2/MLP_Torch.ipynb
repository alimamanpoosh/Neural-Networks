{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure! I can help you with that. Let's start by breaking down the implementation step by step.\n",
    "\n",
    "Step 1: Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Define the MLP model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_1hidden(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer, activations):\n",
    "        hidden_size = hidden_layer[0]\n",
    "        output_size = 10\n",
    "        super(MLP_1hidden, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        if activations[0] == 1:\n",
    "            self.activation_input = nn.ReLU()\n",
    "        elif activations[0] == 2:\n",
    "            self.activation_input = nn.Sigmoid()\n",
    "        \n",
    "        if activations[1] == 1:\n",
    "            self.activation_hidden1 = nn.ReLU()\n",
    "        elif activations[1] == 2:\n",
    "            self.activation_hidden1 = nn.Sigmoid()\n",
    "        \n",
    "        self.activation_output  = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation_input(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation_hidden1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation_output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP_2hidden(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer, activations):\n",
    "        super(MLP, self).__init__()\n",
    "        hidden1_size = hidden_layer[0]\n",
    "        hidden2_size = hidden_layer[1]\n",
    "        output_size = 10\n",
    "        self.fc1 = nn.Linear(input_size, hidden1_size)\n",
    "        self.fc2 = nn.Linear(hidden1_size, hidden2_size)\n",
    "        self.fc3 = nn.Linear(hidden2_size, output_size)\n",
    "        \n",
    "        if activations[0] == 1:\n",
    "            self.activation_input = nn.ReLU()\n",
    "        elif activations[0] == 2:\n",
    "            self.activation_input = nn.Sigmoid()\n",
    "            \n",
    "        if activations[1] == 1 :\n",
    "            self.activation_hidden1 = nn.ReLU()\n",
    "        elif activations[1] == 2:\n",
    "            self.activation_hidden1 = nn.Sigmoid()\n",
    "        \n",
    "        if activations[2] == 1:\n",
    "            self.activation_hidden2 = nn.ReLU()\n",
    "        elif activations[2] == 2:\n",
    "            self.activation_hidden2 = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.activation_output = nn.Softmax(dim=1)        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation_input(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation_hidden1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation_hidden2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.activation_output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_0hidden(nn.Module):\n",
    "    def __init__(self, input_size, output_size, activation):\n",
    "        super(MLP_0hidden, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, output_size)\n",
    "        # self.activation_output = nn.ReLU() if torch.rand(1) > 0.5 else nn.Sigmoid()\n",
    "        if activation[0] == 1:\n",
    "            self.activation_input = nn.ReLU()\n",
    "        elif activation[1] == 2:\n",
    "            self.activation_input = nn.Sigmoid()\n",
    "        self.activation_output = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation_input(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation_output(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Set random seed for reproducibility (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Load the CIFAR-10 dataset and apply transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Create an instance of the MLP model with random configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 512  # Size of the ResNet output feature vector\n",
    "# hidden_size = torch.randint(10, 21, size=(1,)).item()  # Randomly select hidden layer size between 10 and 20\n",
    "\n",
    "hidden_size = torch.choice([0, 10, 20], size=(1,)).item() # Randomly select hidden layer size choice 0 or 10 or 20\n",
    "\n",
    "output_size = 10  # Number of classes in CIFAR-10\n",
    "\n",
    "#chromosome\n",
    "temp = [{'feature extraction': 1, 'hidden layer': [], 'activation function': []}, {'feature extraction': 3, 'hidden layer': [30], 'activation function': [2]}, {'feature extraction': 3, 'hidden layer': [], 'activation function': []}, {'feature extraction': 3, 'hidden layer': [10, 30], 'activation function': [2, 1]}, {'feature extraction': 2, 'hidden layer': [10], 'activation function': [2]}, {'feature extraction': 3, 'hidden layer': [], 'activation function': []}]\n",
    "\n",
    "\n",
    "# model = MLP(input_size, hidden_size, output_size, activation_hidden1, activation_hidden2, activation_output )\n",
    "\n",
    "model = MLP_0hidden(input_size, output_size)\n",
    "molde = MLP_2hidden(input_size, temp[3]['hidden layer'], temp[3]['activation function'])\n",
    "model = MLP_1hidden(input_size, temp[1]['hidden layer'], temp[1]['activation function'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Load a pre-trained ResNet model (either ResNet18 or ResNet34) for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = torch.rand(3)\n",
    "if rand > 2:\n",
    "    model = models.vgg11(pretrained=True)\n",
    "    model = nn.Sequential(*list(model.features)[:-1]) \n",
    "elif rand > 1:\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    model = nn.Sequential(*list(resnet_model.children())[:-1]) \n",
    "else:\n",
    "    model = models.resnet34(pretrained=True)\n",
    "    model = nn.Sequential(*list(resnet_model.children())[:-1]) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Define the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Feature extraction using the pre-trained ResNet model\n",
    "        with torch.no_grad():\n",
    "            features = resnet_model(images)\n",
    "            features = features.view(features.size(0), -1)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "train_loss /= len(train_loader)\n",
    "train_accuracy = correct / total\n",
    "\n",
    "print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "      f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! Let's go through each step in more detail:\n",
    "\n",
    "Step 1: Import the required libraries\n",
    "In this step, we import the necessary libraries for our implementation. We import torch for PyTorch functionalities, torch.nn for defining neural network modules, and torchvision for accessing pre-trained models and datasets.\n",
    "\n",
    "Step 2: Define the MLP model class\n",
    "Here, we define the MLP class which inherits from nn.Module. This class represents our MLP model. It has three fully connected layers (fc1, fc2, fc3) with sizes input_size, hidden_size, and output_size, respectively. The activation function (activation) is randomly chosen to be either nn.ReLU() or nn.Sigmoid() based on a random number generated using torch.rand(1). The forward method defines the forward pass of the model.\n",
    "\n",
    "Step 3: Set random seed for reproducibility (optional)\n",
    "Setting a random seed ensures that the random processes in your code produce the same results every time you run it. It's optional, but it can be useful for reproducibility.\n",
    "\n",
    "Step 4: Load the CIFAR-10 dataset and apply transformations\n",
    "In this step, we define the transformations to be applied to the CIFAR-10 dataset. We use transforms.ToTensor() to convert the images into tensors, and transforms.Normalize() to normalize the image tensors. Then, we create train_dataset and test_dataset by loading the CIFAR-10 dataset from the disk and applying the defined transformations. Finally, we create data loaders (train_loader and test_loader) to load the data in batches for training and testing.\n",
    "\n",
    "Step 5: Create an instance of the MLP model with random configurations\n",
    "Here, we define the input size to be 512, which corresponds to the size of the output feature vector from the ResNet model. We randomly select the hidden layer size between 10 and 20 using torch.randint(). The output size is set to 10, which is the number of classes in CIFAR-10. Finally, we create an instance of the MLP model using the randomly selected configurations.\n",
    "\n",
    "Step 6: Load a pre-trained ResNet model (either ResNet18 or ResNet34) for feature extraction\n",
    "In this step, we randomly select either ResNet18 or ResNet34 using torch.rand(1). Then, we load the chosen pre-trained ResNet model using models.resnet18(pretrained=True) or models.resnet34(pretrained=True). We remove the last fully connected layer from the model using nn.Sequential() and list(resnet_model.children())[:-1]. This modified ResNet model will be used for feature extraction.\n",
    "\n",
    "Step 7: Define the training loop\n",
    "Here, we define the training loop for our model. We set the loss criterion as nn.CrossEntropyLoss() and the optimizer as torch.optim.Adam(). We check if CUDA is available and move the model to the corresponding device (either GPU or CPU). Then, we iterate over the specified number of epochs.\n",
    "\n",
    "Within each epoch, we set the model to training mode using model.train(). We initialize variables to keep track of the loss, correct predictions, and total examples. For each batch in the training data, we move the images and labels to the device, perform feature extraction using the pre-trained ResNet model (with gradients turned off using torch.no_grad()), and pass the extracted features through the MLP model. We calculate the loss, perform backpropagation, and update the model's parameters. Additionally, we calculate the training accuracy by comparing the predicted labels with the true labels.\n",
    "\n",
    "After each epoch,\n",
    "\n",
    "we calculate the average loss and training accuracy. Finally, we print the training loss and accuracy for that epoch.\n",
    "\n",
    "That's a detailed explanation of each step in the implementation. Let me know if you have any further questions!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
