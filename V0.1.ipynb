{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyND5ZVnyEaF2v1Gzk95n7rn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":20,"metadata":{"id":"8sRVUQTFxwpu","executionInfo":{"status":"ok","timestamp":1684320688223,"user_tz":-210,"elapsed":1238,"user":{"displayName":"mohammad mahdavi","userId":"03786949263036795128"}}},"outputs":[],"source":["import numpy as np\n","\n","class MultiClassMLP:\n","    def __init__(self, input_dim, hidden_dims, output_dim, activation='relu', random_seed=42):\n","        self.input_dim = input_dim\n","        self.hidden_dims = hidden_dims\n","        self.output_dim = output_dim\n","        self.layers = []\n","        self.num_layers = len(hidden_dims) + 1\n","        \n","        # Initialize weights and biases for each layer\n","        np.random.seed(random_seed)\n","        layer_dims = [input_dim] + hidden_dims + [output_dim]\n","        self.activations = [self._sigmoid if activation == 'sigmoid' else self._tanh if activation == 'tanh' else self._relu for i in range(self.num_layers)]\n","        for i in range(self.num_layers):\n","            fan_in = layer_dims[i]\n","            fan_out = layer_dims[i+1]\n","            W = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in)\n","            b = np.zeros((1, fan_out))\n","            self.layers.append({'W': W, 'b': b})\n","        \n","    def _softmax(self, X):\n","        exps = np.exp(X - np.max(X, axis=1, keepdims=True))\n","        return exps / np.sum(exps, axis=1, keepdims=True)\n","    \n","    def _tanh(self, X):\n","        return np.tanh(X)\n","    \n","    def _sigmoid(self, X):\n","        return 1 / (1 + np.exp(-X))\n","    \n","    def _relu(self, X):\n","        return np.maximum(0, X)\n","\n","    def delta_cross_entropy(self, X, y):\n","        \"\"\"\n","        X is the output from fully connected layer (num_examples x num_classes)\n","        y is labels (num_examples x 1)\n","            Note that y is not one-hot encoded vector.\n","            It can be computed as y.argmax(axis=1) from one-hot encoded vectors of labels if required.\n","        \"\"\"\n","        m = y.shape[0]\n","        grad = self._softmax(X)\n","        grad[range(m), y] -= 1\n","        grad = grad / m\n","        return grad\n","    \n","    def _forward(self, X):\n","        activations = [X]\n","        for i in range(self.num_layers):\n","            Z = np.dot(activations[-1], self.layers[i]['W']) + self.layers[i]['b']\n","            A = self.activations[i](Z)\n","            activations.append(A)\n","        probs = self._softmax(activations[-1]) # Using softmax activation function for output layer\n","        return activations, probs\n","    \n","    def _backward(self, X, y, activations, probs, learning_rate):\n","        dL_dO = probs - y\n","        for i in reversed(range(self.num_layers)):\n","            if self.activations[i] == self._sigmoid:\n","                dA = dL_dO * activations[i+1] * (1 - activations[i+1])\n","            elif self.activations[i] == self._tanh:\n","                dA = dL_dO * (1 - activations[i+1]**2) # Derivative of tanh activation function\n","            elif self.activations[i] == self._relu:\n","                dA = dL_dO * np.where(activations[i+1] > 0, 1, 0) # Derivative of ReLU activation function\n","            else:\n","                raise ValueError(\"Invalid activation function\")\n","                \n","            dZ = np.dot(dA, self.layers[i]['W'].T)\n","            dW = np.dot(activations[i].T, dA)\n","            db = np.sum(dA, axis=0, keepdims=True)\n","            self.layers[i]['W'] -= learning_rate * dW\n","            self.layers[i]['b'] -= learning_rate * db\n","            \n","            dL_dO = dZ\n","        \n","    def train(self, X_train, y_train, learning_rate=0.1, num_epochs=100):\n","        Y_train = np.eye(self.output_dim)[y_train]\n","        for epoch in range(num_epochs):\n","            activations, probs = self._forward(X_train)\n","            self._backward(X_train, Y_train, activations, probs, learning_rate)\n","            loss = -np.sum(Y_train * np.log(probs)) / X_train.shape[0]\n","            if epoch % 10 == 0:\n","                print(f\"Epoch {epoch}: Loss={loss:.4f}\")\n","                \n","    def predict_proba(self, X):\n","        _, probs = self._forward(X)\n","        return probs\n","    \n","    def predict(self, X):\n","        probs = self.predict_proba(X)\n","        return np.argmax(probs, axis=1)\n","\n","\n"]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","# torch is just for the feature extractor and the dataset (NOT FOR IMPLEMENTING NEURAL NETWORKS!)\n","import torch\n","from torchvision import datasets\n","import torchvision.transforms as transforms\n","from torchvision.models import resnet34\n","import torch.nn as nn\n","# sklearn is just for evaluation (NOT FOR IMPLEMENTING NEURAL NETWORKS!)\n","from sklearn.metrics import confusion_matrix, f1_score"],"metadata":{"id":"tRSiDskIxyDA","executionInfo":{"status":"ok","timestamp":1684320258596,"user_tz":-210,"elapsed":7862,"user":{"displayName":"mohammad mahdavi","userId":"03786949263036795128"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","    ])\n","feature_extractor = resnet34(pretrained=True)\n","input_dim = feature_extractor.fc.in_features\n","for param in feature_extractor.parameters():\n","  param.requires_grad = False\n","\n","feature_extractor.fc = nn.Identity()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6qMq81U3yFpD","executionInfo":{"status":"ok","timestamp":1684320325121,"user_tz":-210,"elapsed":811,"user":{"displayName":"mohammad mahdavi","userId":"03786949263036795128"}},"outputId":"00530ec4-4c46-4bfa-ab21-ec7c18cd755f"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n","100%|██████████| 83.3M/83.3M [00:00<00:00, 230MB/s]\n"]}]},{"cell_type":"code","source":["train_data = datasets.CIFAR10('data', train=True,\n","                              download=True, transform=transform)\n","test_data = datasets.CIFAR10('data', train=False,\n","                             download=True, transform=transform)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UTwdpv59yGBC","executionInfo":{"status":"ok","timestamp":1684320337516,"user_tz":-210,"elapsed":7369,"user":{"displayName":"mohammad mahdavi","userId":"03786949263036795128"}},"outputId":"d396b2cf-5edd-43f5-ab7b-83edce32c5eb"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170498071/170498071 [00:02<00:00, 78155759.13it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/cifar-10-python.tar.gz to data\n","Files already downloaded and verified\n"]}]},{"cell_type":"code","source":["from torch.utils.data import DataLoader"],"metadata":{"id":"Hckp6Q-LyIhY","executionInfo":{"status":"ok","timestamp":1684320343304,"user_tz":-210,"elapsed":595,"user":{"displayName":"mohammad mahdavi","userId":"03786949263036795128"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n","test_loader = DataLoader(test_data, batch_size=64, shuffle=True)"],"metadata":{"id":"4oLQvp-OyLeT","executionInfo":{"status":"ok","timestamp":1684320345450,"user_tz":-210,"elapsed":2,"user":{"displayName":"mohammad mahdavi","userId":"03786949263036795128"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["embeddings = []\n","labels = []\n","i = 0\n","for x, y in train_loader:\n","  if i == 50:\n","    break\n","  i += 1\n","  print(i)\n","  embeddings += feature_extractor(x)\n","  labels += y"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DqxXXgD8yN1b","executionInfo":{"status":"ok","timestamp":1684320364141,"user_tz":-210,"elapsed":16442,"user":{"displayName":"mohammad mahdavi","userId":"03786949263036795128"}},"outputId":"69e729d1-955c-4f04-9692-fe2d80bb7d68"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n","27\n","28\n","29\n","30\n","31\n","32\n","33\n","34\n","35\n","36\n","37\n","38\n","39\n","40\n","41\n","42\n","43\n","44\n","45\n","46\n","47\n","48\n","49\n","50\n"]}]},{"cell_type":"code","source":[" len(next(iter(train_loader))[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OtvEpsKsy-Re","executionInfo":{"status":"ok","timestamp":1684320367709,"user_tz":-210,"elapsed":513,"user":{"displayName":"mohammad mahdavi","userId":"03786949263036795128"}},"outputId":"9129b5ec-c7d5-4309-ce86-dc6dda34ed69"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["64"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["len(embeddings)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qmqQtpx1zd9V","executionInfo":{"status":"ok","timestamp":1684320370733,"user_tz":-210,"elapsed":366,"user":{"displayName":"mohammad mahdavi","userId":"03786949263036795128"}},"outputId":"cb845816-0c22-4e93-daf5-c689fa573af4"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3200"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["labels"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NAT7vrjl49wl","executionInfo":{"status":"ok","timestamp":1684320371027,"user_tz":-210,"elapsed":2,"user":{"displayName":"mohammad mahdavi","userId":"03786949263036795128"}},"outputId":"29d3ec30-62e8-4cc7-a00f-3d031feaf4af"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[tensor(9),\n"," tensor(9),\n"," tensor(4),\n"," tensor(1),\n"," tensor(6),\n"," tensor(2),\n"," tensor(6),\n"," tensor(1),\n"," tensor(9),\n"," tensor(3),\n"," tensor(3),\n"," tensor(5),\n"," tensor(3),\n"," tensor(8),\n"," tensor(5),\n"," tensor(1),\n"," tensor(8),\n"," tensor(4),\n"," tensor(5),\n"," tensor(1),\n"," tensor(2),\n"," tensor(3),\n"," tensor(6),\n"," tensor(7),\n"," tensor(5),\n"," tensor(7),\n"," tensor(1),\n"," tensor(1),\n"," tensor(9),\n"," tensor(7),\n"," tensor(7),\n"," tensor(1),\n"," tensor(6),\n"," tensor(3),\n"," tensor(8),\n"," tensor(8),\n"," tensor(9),\n"," tensor(0),\n"," tensor(1),\n"," tensor(9),\n"," tensor(8),\n"," tensor(1),\n"," tensor(9),\n"," tensor(1),\n"," tensor(4),\n"," tensor(0),\n"," tensor(9),\n"," tensor(4),\n"," tensor(7),\n"," tensor(6),\n"," tensor(3),\n"," tensor(6),\n"," tensor(8),\n"," tensor(4),\n"," tensor(0),\n"," tensor(5),\n"," tensor(5),\n"," tensor(8),\n"," tensor(9),\n"," tensor(1),\n"," tensor(8),\n"," tensor(7),\n"," tensor(7),\n"," tensor(6),\n"," tensor(2),\n"," tensor(2),\n"," tensor(8),\n"," tensor(0),\n"," tensor(9),\n"," tensor(2),\n"," tensor(7),\n"," tensor(0),\n"," tensor(2),\n"," tensor(0),\n"," tensor(4),\n"," tensor(4),\n"," tensor(6),\n"," tensor(1),\n"," tensor(5),\n"," tensor(4),\n"," tensor(9),\n"," tensor(0),\n"," tensor(0),\n"," tensor(6),\n"," tensor(2),\n"," tensor(9),\n"," tensor(1),\n"," tensor(2),\n"," tensor(2),\n"," tensor(3),\n"," tensor(6),\n"," tensor(9),\n"," tensor(0),\n"," tensor(2),\n"," tensor(4),\n"," tensor(4),\n"," tensor(1),\n"," tensor(2),\n"," tensor(9),\n"," tensor(6),\n"," tensor(9),\n"," tensor(4),\n"," tensor(4),\n"," tensor(5),\n"," tensor(3),\n"," tensor(5),\n"," tensor(6),\n"," tensor(8),\n"," tensor(1),\n"," tensor(9),\n"," tensor(9),\n"," tensor(2),\n"," tensor(8),\n"," tensor(6),\n"," tensor(3),\n"," tensor(6),\n"," tensor(3),\n"," tensor(8),\n"," tensor(7),\n"," tensor(2),\n"," tensor(2),\n"," tensor(0),\n"," tensor(1),\n"," tensor(7),\n"," tensor(3),\n"," tensor(9),\n"," tensor(5),\n"," tensor(1),\n"," tensor(2),\n"," tensor(1),\n"," tensor(9),\n"," tensor(0),\n"," tensor(8),\n"," tensor(8),\n"," tensor(2),\n"," tensor(4),\n"," tensor(9),\n"," tensor(1),\n"," tensor(9),\n"," tensor(0),\n"," tensor(2),\n"," tensor(0),\n"," tensor(1),\n"," tensor(8),\n"," tensor(8),\n"," tensor(4),\n"," tensor(3),\n"," tensor(8),\n"," tensor(4),\n"," tensor(5),\n"," tensor(0),\n"," tensor(3),\n"," tensor(8),\n"," tensor(1),\n"," tensor(1),\n"," tensor(7),\n"," tensor(4),\n"," tensor(9),\n"," tensor(1),\n"," tensor(5),\n"," tensor(9),\n"," tensor(9),\n"," tensor(9),\n"," tensor(4),\n"," tensor(5),\n"," tensor(6),\n"," tensor(7),\n"," tensor(9),\n"," tensor(6),\n"," tensor(4),\n"," tensor(5),\n"," tensor(8),\n"," tensor(7),\n"," tensor(0),\n"," tensor(5),\n"," tensor(7),\n"," tensor(9),\n"," tensor(1),\n"," tensor(5),\n"," tensor(9),\n"," tensor(1),\n"," tensor(2),\n"," tensor(6),\n"," tensor(0),\n"," tensor(0),\n"," tensor(6),\n"," tensor(1),\n"," tensor(4),\n"," tensor(2),\n"," tensor(2),\n"," tensor(9),\n"," tensor(7),\n"," tensor(4),\n"," tensor(2),\n"," tensor(5),\n"," tensor(7),\n"," tensor(7),\n"," tensor(4),\n"," tensor(3),\n"," tensor(2),\n"," tensor(4),\n"," tensor(2),\n"," tensor(5),\n"," tensor(2),\n"," tensor(9),\n"," tensor(5),\n"," tensor(7),\n"," tensor(1),\n"," tensor(1),\n"," tensor(9),\n"," tensor(7),\n"," tensor(3),\n"," tensor(7),\n"," tensor(1),\n"," tensor(5),\n"," tensor(9),\n"," tensor(4),\n"," tensor(9),\n"," tensor(7),\n"," tensor(9),\n"," tensor(7),\n"," tensor(9),\n"," tensor(2),\n"," tensor(6),\n"," tensor(7),\n"," tensor(2),\n"," tensor(2),\n"," tensor(3),\n"," tensor(0),\n"," tensor(1),\n"," tensor(1),\n"," tensor(6),\n"," tensor(4),\n"," tensor(8),\n"," tensor(8),\n"," tensor(3),\n"," tensor(9),\n"," tensor(8),\n"," tensor(9),\n"," tensor(6),\n"," tensor(4),\n"," tensor(4),\n"," tensor(7),\n"," tensor(9),\n"," tensor(7),\n"," tensor(8),\n"," tensor(4),\n"," tensor(3),\n"," tensor(2),\n"," tensor(9),\n"," tensor(6),\n"," tensor(0),\n"," tensor(1),\n"," tensor(0),\n"," tensor(7),\n"," tensor(6),\n"," tensor(6),\n"," tensor(0),\n"," tensor(0),\n"," tensor(4),\n"," tensor(5),\n"," tensor(2),\n"," tensor(7),\n"," tensor(6),\n"," tensor(9),\n"," tensor(8),\n"," tensor(1),\n"," tensor(6),\n"," tensor(1),\n"," tensor(2),\n"," tensor(7),\n"," tensor(2),\n"," tensor(4),\n"," tensor(1),\n"," tensor(7),\n"," tensor(5),\n"," tensor(0),\n"," tensor(0),\n"," tensor(9),\n"," tensor(8),\n"," tensor(5),\n"," tensor(1),\n"," tensor(5),\n"," tensor(1),\n"," tensor(9),\n"," tensor(2),\n"," tensor(0),\n"," tensor(9),\n"," tensor(5),\n"," tensor(5),\n"," tensor(5),\n"," tensor(9),\n"," tensor(9),\n"," tensor(3),\n"," tensor(1),\n"," tensor(7),\n"," tensor(2),\n"," tensor(5),\n"," tensor(9),\n"," tensor(9),\n"," tensor(7),\n"," tensor(7),\n"," tensor(9),\n"," tensor(0),\n"," tensor(8),\n"," tensor(4),\n"," tensor(5),\n"," tensor(8),\n"," tensor(3),\n"," tensor(4),\n"," tensor(1),\n"," tensor(8),\n"," tensor(8),\n"," tensor(7),\n"," tensor(9),\n"," tensor(8),\n"," tensor(6),\n"," tensor(8),\n"," tensor(1),\n"," tensor(7),\n"," tensor(7),\n"," tensor(5),\n"," tensor(1),\n"," tensor(7),\n"," tensor(7),\n"," tensor(2),\n"," tensor(2),\n"," tensor(9),\n"," tensor(4),\n"," tensor(5),\n"," tensor(1),\n"," tensor(9),\n"," tensor(3),\n"," tensor(5),\n"," tensor(2),\n"," tensor(0),\n"," tensor(1),\n"," tensor(0),\n"," tensor(5),\n"," tensor(7),\n"," tensor(0),\n"," tensor(8),\n"," tensor(9),\n"," tensor(6),\n"," tensor(0),\n"," tensor(3),\n"," tensor(0),\n"," tensor(7),\n"," tensor(2),\n"," tensor(7),\n"," tensor(2),\n"," tensor(1),\n"," tensor(1),\n"," tensor(8),\n"," tensor(2),\n"," tensor(9),\n"," tensor(4),\n"," tensor(5),\n"," tensor(8),\n"," tensor(8),\n"," tensor(4),\n"," tensor(4),\n"," tensor(9),\n"," tensor(6),\n"," tensor(3),\n"," tensor(1),\n"," tensor(9),\n"," tensor(4),\n"," tensor(3),\n"," tensor(0),\n"," tensor(1),\n"," tensor(7),\n"," tensor(1),\n"," tensor(4),\n"," tensor(5),\n"," tensor(4),\n"," tensor(6),\n"," tensor(1),\n"," tensor(2),\n"," tensor(4),\n"," tensor(6),\n"," tensor(6),\n"," tensor(9),\n"," tensor(8),\n"," tensor(1),\n"," tensor(9),\n"," tensor(0),\n"," tensor(6),\n"," tensor(2),\n"," tensor(0),\n"," tensor(7),\n"," tensor(5),\n"," tensor(7),\n"," tensor(3),\n"," tensor(2),\n"," tensor(0),\n"," tensor(0),\n"," tensor(8),\n"," tensor(6),\n"," tensor(0),\n"," tensor(9),\n"," tensor(5),\n"," tensor(5),\n"," tensor(0),\n"," tensor(1),\n"," tensor(4),\n"," tensor(2),\n"," tensor(3),\n"," tensor(0),\n"," tensor(3),\n"," tensor(4),\n"," tensor(3),\n"," tensor(5),\n"," tensor(6),\n"," tensor(2),\n"," tensor(4),\n"," tensor(7),\n"," tensor(8),\n"," tensor(2),\n"," tensor(9),\n"," tensor(7),\n"," tensor(8),\n"," tensor(1),\n"," tensor(4),\n"," tensor(8),\n"," tensor(7),\n"," tensor(5),\n"," tensor(5),\n"," tensor(1),\n"," tensor(2),\n"," tensor(4),\n"," tensor(2),\n"," tensor(9),\n"," tensor(8),\n"," tensor(3),\n"," tensor(8),\n"," tensor(1),\n"," tensor(7),\n"," tensor(2),\n"," tensor(3),\n"," tensor(2),\n"," tensor(4),\n"," tensor(6),\n"," tensor(3),\n"," tensor(2),\n"," tensor(0),\n"," tensor(4),\n"," tensor(9),\n"," tensor(8),\n"," tensor(1),\n"," tensor(3),\n"," tensor(5),\n"," tensor(8),\n"," tensor(1),\n"," tensor(2),\n"," tensor(4),\n"," tensor(7),\n"," tensor(8),\n"," tensor(0),\n"," tensor(9),\n"," tensor(7),\n"," tensor(0),\n"," tensor(6),\n"," tensor(2),\n"," tensor(6),\n"," tensor(2),\n"," tensor(8),\n"," tensor(4),\n"," tensor(5),\n"," tensor(1),\n"," tensor(7),\n"," tensor(1),\n"," tensor(2),\n"," tensor(9),\n"," tensor(5),\n"," tensor(7),\n"," tensor(7),\n"," tensor(8),\n"," tensor(8),\n"," tensor(0),\n"," tensor(3),\n"," tensor(3),\n"," tensor(2),\n"," tensor(7),\n"," tensor(8),\n"," tensor(3),\n"," tensor(9),\n"," tensor(9),\n"," tensor(8),\n"," tensor(8),\n"," tensor(0),\n"," tensor(7),\n"," tensor(1),\n"," tensor(4),\n"," tensor(7),\n"," tensor(4),\n"," tensor(5),\n"," tensor(7),\n"," tensor(0),\n"," tensor(3),\n"," tensor(0),\n"," tensor(7),\n"," tensor(0),\n"," tensor(9),\n"," tensor(3),\n"," tensor(8),\n"," tensor(6),\n"," tensor(6),\n"," tensor(8),\n"," tensor(9),\n"," tensor(8),\n"," tensor(1),\n"," tensor(1),\n"," tensor(9),\n"," tensor(8),\n"," tensor(3),\n"," tensor(8),\n"," tensor(6),\n"," tensor(2),\n"," tensor(5),\n"," tensor(7),\n"," tensor(4),\n"," tensor(8),\n"," tensor(6),\n"," tensor(3),\n"," tensor(5),\n"," tensor(3),\n"," tensor(7),\n"," tensor(5),\n"," tensor(1),\n"," tensor(3),\n"," tensor(8),\n"," tensor(8),\n"," tensor(7),\n"," tensor(8),\n"," tensor(5),\n"," tensor(8),\n"," tensor(2),\n"," tensor(3),\n"," tensor(1),\n"," tensor(7),\n"," tensor(0),\n"," tensor(9),\n"," tensor(5),\n"," tensor(6),\n"," tensor(9),\n"," tensor(4),\n"," tensor(2),\n"," tensor(7),\n"," tensor(3),\n"," tensor(1),\n"," tensor(8),\n"," tensor(1),\n"," tensor(6),\n"," tensor(9),\n"," tensor(4),\n"," tensor(5),\n"," tensor(0),\n"," tensor(7),\n"," tensor(0),\n"," tensor(1),\n"," tensor(5),\n"," tensor(6),\n"," tensor(7),\n"," tensor(3),\n"," tensor(0),\n"," tensor(0),\n"," tensor(5),\n"," tensor(2),\n"," tensor(1),\n"," tensor(0),\n"," tensor(7),\n"," tensor(7),\n"," tensor(3),\n"," tensor(4),\n"," tensor(4),\n"," tensor(4),\n"," tensor(2),\n"," tensor(6),\n"," tensor(2),\n"," tensor(0),\n"," tensor(1),\n"," tensor(6),\n"," tensor(1),\n"," tensor(3),\n"," tensor(2),\n"," tensor(2),\n"," tensor(9),\n"," tensor(5),\n"," tensor(8),\n"," tensor(6),\n"," tensor(9),\n"," tensor(1),\n"," tensor(3),\n"," tensor(6),\n"," tensor(8),\n"," tensor(3),\n"," tensor(2),\n"," tensor(3),\n"," tensor(8),\n"," tensor(0),\n"," tensor(9),\n"," tensor(2),\n"," tensor(4),\n"," tensor(9),\n"," tensor(6),\n"," tensor(0),\n"," tensor(9),\n"," tensor(0),\n"," tensor(7),\n"," tensor(0),\n"," tensor(0),\n"," tensor(4),\n"," tensor(6),\n"," tensor(2),\n"," tensor(1),\n"," tensor(6),\n"," tensor(1),\n"," tensor(1),\n"," tensor(4),\n"," tensor(8),\n"," tensor(8),\n"," tensor(5),\n"," tensor(9),\n"," tensor(2),\n"," tensor(7),\n"," tensor(2),\n"," tensor(7),\n"," tensor(0),\n"," tensor(1),\n"," tensor(1),\n"," tensor(5),\n"," tensor(8),\n"," tensor(7),\n"," tensor(4),\n"," tensor(3),\n"," tensor(6),\n"," tensor(5),\n"," tensor(4),\n"," tensor(1),\n"," tensor(8),\n"," tensor(7),\n"," tensor(7),\n"," tensor(6),\n"," tensor(7),\n"," tensor(3),\n"," tensor(5),\n"," tensor(0),\n"," tensor(9),\n"," tensor(7),\n"," tensor(8),\n"," tensor(6),\n"," tensor(1),\n"," tensor(6),\n"," tensor(5),\n"," tensor(8),\n"," tensor(9),\n"," tensor(3),\n"," tensor(1),\n"," tensor(8),\n"," tensor(9),\n"," tensor(0),\n"," tensor(3),\n"," tensor(7),\n"," tensor(7),\n"," tensor(2),\n"," tensor(8),\n"," tensor(6),\n"," tensor(1),\n"," tensor(8),\n"," tensor(9),\n"," tensor(1),\n"," tensor(5),\n"," tensor(7),\n"," tensor(4),\n"," tensor(6),\n"," tensor(4),\n"," tensor(1),\n"," tensor(7),\n"," tensor(2),\n"," tensor(6),\n"," tensor(8),\n"," tensor(4),\n"," tensor(4),\n"," tensor(3),\n"," tensor(9),\n"," tensor(6),\n"," tensor(4),\n"," tensor(4),\n"," tensor(5),\n"," tensor(1),\n"," tensor(4),\n"," tensor(9),\n"," tensor(9),\n"," tensor(8),\n"," tensor(3),\n"," tensor(1),\n"," tensor(6),\n"," tensor(2),\n"," tensor(0),\n"," tensor(4),\n"," tensor(9),\n"," tensor(9),\n"," tensor(3),\n"," tensor(6),\n"," tensor(2),\n"," tensor(9),\n"," tensor(9),\n"," tensor(3),\n"," tensor(5),\n"," tensor(5),\n"," tensor(8),\n"," tensor(9),\n"," tensor(1),\n"," tensor(0),\n"," tensor(5),\n"," tensor(2),\n"," tensor(7),\n"," tensor(1),\n"," tensor(8),\n"," tensor(7),\n"," tensor(2),\n"," tensor(5),\n"," tensor(2),\n"," tensor(0),\n"," tensor(6),\n"," tensor(1),\n"," tensor(1),\n"," tensor(0),\n"," tensor(6),\n"," tensor(2),\n"," tensor(2),\n"," tensor(8),\n"," tensor(8),\n"," tensor(4),\n"," tensor(3),\n"," tensor(4),\n"," tensor(8),\n"," tensor(8),\n"," tensor(7),\n"," tensor(7),\n"," tensor(2),\n"," tensor(8),\n"," tensor(1),\n"," tensor(7),\n"," tensor(8),\n"," tensor(0),\n"," tensor(6),\n"," tensor(3),\n"," tensor(3),\n"," tensor(9),\n"," tensor(2),\n"," tensor(9),\n"," tensor(0),\n"," tensor(8),\n"," tensor(3),\n"," tensor(8),\n"," tensor(9),\n"," tensor(8),\n"," tensor(8),\n"," tensor(4),\n"," tensor(7),\n"," tensor(9),\n"," tensor(3),\n"," tensor(1),\n"," tensor(9),\n"," tensor(3),\n"," tensor(8),\n"," tensor(4),\n"," tensor(9),\n"," tensor(5),\n"," tensor(6),\n"," tensor(8),\n"," tensor(2),\n"," tensor(7),\n"," tensor(9),\n"," tensor(3),\n"," tensor(1),\n"," tensor(5),\n"," tensor(5),\n"," tensor(0),\n"," tensor(1),\n"," tensor(7),\n"," tensor(8),\n"," tensor(3),\n"," tensor(5),\n"," tensor(7),\n"," tensor(7),\n"," tensor(7),\n"," tensor(4),\n"," tensor(0),\n"," tensor(9),\n"," tensor(7),\n"," tensor(5),\n"," tensor(8),\n"," tensor(9),\n"," tensor(3),\n"," tensor(1),\n"," tensor(8),\n"," tensor(2),\n"," tensor(2),\n"," tensor(5),\n"," tensor(7),\n"," tensor(1),\n"," tensor(7),\n"," tensor(4),\n"," tensor(4),\n"," tensor(7),\n"," tensor(7),\n"," tensor(5),\n"," tensor(4),\n"," tensor(1),\n"," tensor(5),\n"," tensor(6),\n"," tensor(5),\n"," tensor(3),\n"," tensor(8),\n"," tensor(1),\n"," tensor(5),\n"," tensor(0),\n"," tensor(2),\n"," tensor(6),\n"," tensor(1),\n"," tensor(2),\n"," tensor(1),\n"," tensor(9),\n"," tensor(6),\n"," tensor(3),\n"," tensor(4),\n"," tensor(0),\n"," tensor(5),\n"," tensor(6),\n"," tensor(2),\n"," tensor(2),\n"," tensor(7),\n"," tensor(1),\n"," tensor(1),\n"," tensor(6),\n"," tensor(1),\n"," tensor(2),\n"," tensor(9),\n"," tensor(3),\n"," tensor(4),\n"," tensor(9),\n"," tensor(3),\n"," tensor(7),\n"," tensor(3),\n"," tensor(9),\n"," tensor(7),\n"," tensor(2),\n"," tensor(0),\n"," tensor(4),\n"," tensor(1),\n"," tensor(8),\n"," tensor(1),\n"," tensor(6),\n"," tensor(0),\n"," tensor(2),\n"," tensor(9),\n"," tensor(3),\n"," tensor(5),\n"," tensor(8),\n"," tensor(1),\n"," tensor(7),\n"," tensor(2),\n"," tensor(5),\n"," tensor(6),\n"," tensor(0),\n"," tensor(7),\n"," tensor(8),\n"," tensor(8),\n"," tensor(7),\n"," tensor(0),\n"," tensor(0),\n"," tensor(7),\n"," tensor(3),\n"," tensor(5),\n"," tensor(7),\n"," tensor(2),\n"," tensor(9),\n"," tensor(3),\n"," tensor(7),\n"," tensor(2),\n"," tensor(1),\n"," tensor(4),\n"," tensor(3),\n"," tensor(3),\n"," tensor(0),\n"," tensor(8),\n"," tensor(9),\n"," tensor(1),\n"," tensor(9),\n"," tensor(3),\n"," tensor(9),\n"," tensor(1),\n"," tensor(8),\n"," tensor(8),\n"," tensor(3),\n"," tensor(8),\n"," tensor(4),\n"," tensor(9),\n"," tensor(8),\n"," tensor(6),\n"," tensor(5),\n"," tensor(9),\n"," tensor(6),\n"," tensor(1),\n"," tensor(7),\n"," tensor(5),\n"," tensor(8),\n"," tensor(2),\n"," tensor(0),\n"," tensor(4),\n"," tensor(9),\n"," tensor(5),\n"," tensor(3),\n"," tensor(4),\n"," tensor(2),\n"," tensor(6),\n"," tensor(8),\n"," tensor(6),\n"," tensor(1),\n"," tensor(1),\n"," tensor(3),\n"," tensor(3),\n"," tensor(9),\n"," tensor(6),\n"," tensor(4),\n"," tensor(9),\n"," tensor(4),\n"," tensor(0),\n"," tensor(3),\n"," tensor(1),\n"," tensor(9),\n"," tensor(1),\n"," tensor(0),\n"," tensor(1),\n"," tensor(6),\n"," tensor(9),\n"," tensor(5),\n"," tensor(4),\n"," tensor(7),\n"," tensor(8),\n"," tensor(9),\n"," tensor(9),\n"," tensor(3),\n"," tensor(4),\n"," tensor(1),\n"," tensor(1),\n"," tensor(4),\n"," tensor(7),\n"," tensor(9),\n"," tensor(2),\n"," tensor(7),\n"," tensor(1),\n"," tensor(0),\n"," tensor(1),\n"," tensor(2),\n"," tensor(5),\n"," tensor(8),\n"," tensor(1),\n"," tensor(2),\n"," tensor(7),\n"," tensor(9),\n"," tensor(0),\n"," tensor(8),\n"," tensor(2),\n"," tensor(3),\n"," tensor(3),\n"," tensor(4),\n"," tensor(3),\n"," tensor(2),\n"," tensor(4),\n"," tensor(0),\n"," tensor(3),\n"," tensor(2),\n"," tensor(8),\n"," tensor(1),\n"," tensor(4),\n"," tensor(7),\n"," tensor(6),\n"," tensor(8),\n"," tensor(0),\n"," tensor(6),\n"," tensor(0),\n"," tensor(2),\n"," tensor(4),\n"," tensor(5),\n"," tensor(1),\n"," tensor(6),\n"," tensor(7),\n"," tensor(5),\n"," tensor(4),\n"," tensor(1),\n"," tensor(4),\n"," tensor(1),\n"," tensor(1),\n"," tensor(3),\n"," tensor(2),\n"," tensor(3),\n"," ...]"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["# data_tensor = torch.cat(embeddings, dim=0)\n","\n","# Convert the tensor to a NumPy array\n","X = np.array(embeddings)\n","# data_tensor = torch.cat(labels, dim=0)\n","for i in range(len(X)):\n","  X[i] = np.array(X[i])\n","# Convert the tensor to a NumPy array\n","Y = np.array(labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n9W9yPMj4uql","executionInfo":{"status":"ok","timestamp":1684320374768,"user_tz":-210,"elapsed":1174,"user":{"displayName":"mohammad mahdavi","userId":"03786949263036795128"}},"outputId":"43aeb22a-ff50-41ae-ae47-8aaf1f9d7ab0"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-11-e56cbe7bc52a>:4: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n","  X = np.array(embeddings)\n","<ipython-input-11-e56cbe7bc52a>:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  X = np.array(embeddings)\n"]}]},{"cell_type":"code","source":["result = []\n","for i in X:\n","  result.append(i)\n","result = np.array(result)\n","result.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Txs29uJ5pf2","executionInfo":{"status":"ok","timestamp":1684320377210,"user_tz":-210,"elapsed":1466,"user":{"displayName":"mohammad mahdavi","userId":"03786949263036795128"}},"outputId":"b113b1fe-2c5b-4895-b297-7b3943e7182a"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3200, 512)"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["from sklearn.neural_network import MLPClassifier\n","clf = MLPClassifier(random_state=1, max_iter=1000, hidden_layer_sizes=[20]).fit(result, Y)\n","clf.score(result, Y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2nOciu92XM2k","executionInfo":{"status":"ok","timestamp":1684321260382,"user_tz":-210,"elapsed":11817,"user":{"displayName":"mohammad mahdavi","userId":"03786949263036795128"}},"outputId":"7b505d9a-5cf5-4d5a-a44e-e2af2d89b5e1"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.999375"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["mlp = MultiClassMLP(input_dim=512, hidden_dims=[100], output_dim=10, activation=\"tanh\")\n","mlp.train(result, Y, learning_rate=0.001, num_epochs=1000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ITizemtm2TzR","executionInfo":{"status":"ok","timestamp":1684323333303,"user_tz":-210,"elapsed":77185,"user":{"displayName":"mohammad mahdavi","userId":"03786949263036795128"}},"outputId":"3701ed9d-9db4-4370-ccbb-b8798546e70d"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0: Loss=2.4453\n","Epoch 10: Loss=2.3689\n","Epoch 20: Loss=2.1965\n","Epoch 30: Loss=2.2292\n","Epoch 40: Loss=2.1404\n","Epoch 50: Loss=2.1093\n","Epoch 60: Loss=2.0677\n","Epoch 70: Loss=2.0357\n","Epoch 80: Loss=1.8525\n","Epoch 90: Loss=1.7978\n","Epoch 100: Loss=1.7345\n","Epoch 110: Loss=1.8562\n","Epoch 120: Loss=1.5971\n","Epoch 130: Loss=1.7594\n","Epoch 140: Loss=1.7704\n","Epoch 150: Loss=1.7224\n","Epoch 160: Loss=1.6534\n","Epoch 170: Loss=1.5692\n","Epoch 180: Loss=1.5759\n","Epoch 190: Loss=1.6665\n","Epoch 200: Loss=1.5495\n","Epoch 210: Loss=1.3887\n","Epoch 220: Loss=1.4652\n","Epoch 230: Loss=1.5551\n","Epoch 240: Loss=1.4326\n","Epoch 250: Loss=1.4783\n","Epoch 260: Loss=1.4594\n","Epoch 270: Loss=1.4211\n","Epoch 280: Loss=1.2683\n","Epoch 290: Loss=1.4031\n","Epoch 300: Loss=1.2291\n","Epoch 310: Loss=1.6110\n","Epoch 320: Loss=1.3188\n","Epoch 330: Loss=1.3167\n","Epoch 340: Loss=1.2982\n","Epoch 350: Loss=1.2409\n","Epoch 360: Loss=1.2686\n","Epoch 370: Loss=1.2485\n","Epoch 380: Loss=1.2787\n","Epoch 390: Loss=1.2009\n","Epoch 400: Loss=1.2387\n","Epoch 410: Loss=1.1639\n","Epoch 420: Loss=1.1168\n","Epoch 430: Loss=1.0988\n","Epoch 440: Loss=1.1338\n","Epoch 450: Loss=1.1465\n","Epoch 460: Loss=1.0664\n","Epoch 470: Loss=1.2735\n","Epoch 480: Loss=1.3594\n","Epoch 490: Loss=1.2127\n","Epoch 500: Loss=1.1023\n","Epoch 510: Loss=1.0643\n","Epoch 520: Loss=1.1129\n","Epoch 530: Loss=1.1487\n","Epoch 540: Loss=1.1731\n","Epoch 550: Loss=0.9795\n","Epoch 560: Loss=0.9779\n","Epoch 570: Loss=1.0923\n","Epoch 580: Loss=1.0722\n","Epoch 590: Loss=1.1869\n","Epoch 600: Loss=1.1146\n","Epoch 610: Loss=0.9694\n","Epoch 620: Loss=0.9604\n","Epoch 630: Loss=0.9557\n","Epoch 640: Loss=0.9525\n","Epoch 650: Loss=0.9500\n","Epoch 660: Loss=0.9481\n","Epoch 670: Loss=0.9464\n","Epoch 680: Loss=0.9450\n","Epoch 690: Loss=0.9440\n","Epoch 700: Loss=0.9429\n","Epoch 710: Loss=0.9420\n","Epoch 720: Loss=0.9412\n","Epoch 730: Loss=0.9404\n","Epoch 740: Loss=0.9396\n","Epoch 750: Loss=0.9389\n","Epoch 760: Loss=0.9381\n","Epoch 770: Loss=0.9374\n","Epoch 780: Loss=0.9368\n","Epoch 790: Loss=0.9359\n","Epoch 800: Loss=0.9354\n","Epoch 810: Loss=0.9350\n","Epoch 820: Loss=0.9347\n","Epoch 830: Loss=0.9342\n","Epoch 840: Loss=0.9337\n","Epoch 850: Loss=0.9332\n","Epoch 860: Loss=0.9328\n","Epoch 870: Loss=0.9324\n","Epoch 880: Loss=0.9321\n","Epoch 890: Loss=0.9318\n","Epoch 900: Loss=0.9315\n","Epoch 910: Loss=0.9312\n","Epoch 920: Loss=0.9308\n","Epoch 930: Loss=0.9304\n","Epoch 940: Loss=0.9298\n","Epoch 950: Loss=0.9294\n","Epoch 960: Loss=0.9289\n","Epoch 970: Loss=0.9285\n","Epoch 980: Loss=0.9278\n","Epoch 990: Loss=0.9273\n"]}]},{"cell_type":"code","source":["counter = 0\n","for i, t in enumerate(result):\n","  if mlp.predict(t) == Y[i]:\n","    counter += 1\n","print(counter / len(result))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ArEkZbY-f8xY","executionInfo":{"status":"ok","timestamp":1684323641657,"user_tz":-210,"elapsed":416,"user":{"displayName":"mohammad mahdavi","userId":"03786949263036795128"}},"outputId":"8808fd08-00bf-4900-9fab-2a9fc4b5650f"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["0.8690625\n"]}]},{"cell_type":"code","source":["X_train = np.random.randn(100, 10)\n","y_train = np.random.randint(0, 5, size=100)\n","# Y_train = np.eye(5)[y_train]\n","\n","# Initialize and train the MLP\n","mlp2 = MultiClassMLP(input_dim=10, hidden_dims=[20], output_dim=5, activation=\"sigmoid\")\n","mlp2.train(X_train, y_train, learning_rate=0.1, num_epochs=1000)"],"metadata":{"id":"lel9G6Ip3cYg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684320789952,"user_tz":-210,"elapsed":737,"user":{"displayName":"mohammad mahdavi","userId":"03786949263036795128"}},"outputId":"5f75c4a1-fdfb-4344-f8a6-783b93aa7bd0"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0: Loss=1.5996\n","Epoch 10: Loss=1.5479\n","Epoch 20: Loss=1.5098\n","Epoch 30: Loss=1.4817\n","Epoch 40: Loss=1.4593\n","Epoch 50: Loss=1.4382\n","Epoch 60: Loss=1.4175\n","Epoch 70: Loss=1.3972\n","Epoch 80: Loss=1.3778\n","Epoch 90: Loss=1.3594\n","Epoch 100: Loss=1.3424\n","Epoch 110: Loss=1.3268\n","Epoch 120: Loss=1.3122\n","Epoch 130: Loss=1.2984\n","Epoch 140: Loss=1.2854\n","Epoch 150: Loss=1.2729\n","Epoch 160: Loss=1.2605\n","Epoch 170: Loss=1.2485\n","Epoch 180: Loss=1.2379\n","Epoch 190: Loss=1.2283\n","Epoch 200: Loss=1.2195\n","Epoch 210: Loss=1.2114\n","Epoch 220: Loss=1.2039\n","Epoch 230: Loss=1.1970\n","Epoch 240: Loss=1.1905\n","Epoch 250: Loss=1.1842\n","Epoch 260: Loss=1.1781\n","Epoch 270: Loss=1.1726\n","Epoch 280: Loss=1.1675\n","Epoch 290: Loss=1.1631\n","Epoch 300: Loss=1.1593\n","Epoch 310: Loss=1.1560\n","Epoch 320: Loss=1.1531\n","Epoch 330: Loss=1.1504\n","Epoch 340: Loss=1.1480\n","Epoch 350: Loss=1.1458\n","Epoch 360: Loss=1.1438\n","Epoch 370: Loss=1.1418\n","Epoch 380: Loss=1.1399\n","Epoch 390: Loss=1.1381\n","Epoch 400: Loss=1.1364\n","Epoch 410: Loss=1.1347\n","Epoch 420: Loss=1.1332\n","Epoch 430: Loss=1.1318\n","Epoch 440: Loss=1.1305\n","Epoch 450: Loss=1.1293\n","Epoch 460: Loss=1.1280\n","Epoch 470: Loss=1.1248\n","Epoch 480: Loss=1.1216\n","Epoch 490: Loss=1.1203\n","Epoch 500: Loss=1.1192\n","Epoch 510: Loss=1.1182\n","Epoch 520: Loss=1.1174\n","Epoch 530: Loss=1.1165\n","Epoch 540: Loss=1.1157\n","Epoch 550: Loss=1.1146\n","Epoch 560: Loss=1.1134\n","Epoch 570: Loss=1.1126\n","Epoch 580: Loss=1.1118\n","Epoch 590: Loss=1.1111\n","Epoch 600: Loss=1.1101\n","Epoch 610: Loss=1.1090\n","Epoch 620: Loss=1.1081\n","Epoch 630: Loss=1.1073\n","Epoch 640: Loss=1.1066\n","Epoch 650: Loss=1.1060\n","Epoch 660: Loss=1.1054\n","Epoch 670: Loss=1.1049\n","Epoch 680: Loss=1.1043\n","Epoch 690: Loss=1.1038\n","Epoch 700: Loss=1.1032\n","Epoch 710: Loss=1.1024\n","Epoch 720: Loss=1.1011\n","Epoch 730: Loss=1.1000\n","Epoch 740: Loss=1.0993\n","Epoch 750: Loss=1.0987\n","Epoch 760: Loss=1.0982\n","Epoch 770: Loss=1.0976\n","Epoch 780: Loss=1.0972\n","Epoch 790: Loss=1.0967\n","Epoch 800: Loss=1.0963\n","Epoch 810: Loss=1.0959\n","Epoch 820: Loss=1.0955\n","Epoch 830: Loss=1.0951\n","Epoch 840: Loss=1.0947\n","Epoch 850: Loss=1.0942\n","Epoch 860: Loss=1.0936\n","Epoch 870: Loss=1.0930\n","Epoch 880: Loss=1.0926\n","Epoch 890: Loss=1.0922\n","Epoch 900: Loss=1.0919\n","Epoch 910: Loss=1.0916\n","Epoch 920: Loss=1.0912\n","Epoch 930: Loss=1.0908\n","Epoch 940: Loss=1.0903\n","Epoch 950: Loss=1.0899\n","Epoch 960: Loss=1.0896\n","Epoch 970: Loss=1.0894\n","Epoch 980: Loss=1.0891\n","Epoch 990: Loss=1.0889\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"QTcr5l_7Ufnc"},"execution_count":null,"outputs":[]}]}